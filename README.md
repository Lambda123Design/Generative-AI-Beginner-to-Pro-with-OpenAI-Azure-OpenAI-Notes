# Generative-AI-Beginner-to-Pro-with-OpenAI-Azure-OpenAI-Notes

**I) AI Concepts and Workloads - (For Absolute Beginners- Optional)**

**A) What is AI ?**

**B) History of AI**

**C) Benefits of Artificial Intelligence (AI)**

**D) Types of AI Workloads**

**E) AI vs ML vs DL**

**II) Machine Learning Foundations (For Absolute Beginners-Optional)**

**A) Machine Learning - Real World Example**

**B) Machine Learning - Key Terminologies**

**C) What is Machine Learning ?**

**D) Types of Machine Learning**

**E) What is Supervised Machine Learning ?**

**F) What is Classification in SML ?**

**G) What is Regression in SML ?**

**H) What is Unsupervised Machine Learning ?**

**I) What is Re-inforcement Machine Learning**

**J) What is a Jupyter Notebook**

**K) Demo: Install Anaconda**

**L) Demo: Understanding the IRIS Dataset**

**M) Demo: Creating & Training your ML Model**

**III) Deep Learning Foundations (For Absolute Beginners- Optional)**

**A) What is Deep Learning ?**

**B) What is a Neural Network ?**

**C) Deep Learning Models**

**D) What is a Transformer Model?**

**E) Demo: GANs - Deep Fake Video**

**F) Demo- Creating & Training Deep Learning Model**

**IV) Generative AI Foundations (For Beginners)**

**A) What is Generative AI ?**

**B) Predictive AI vs Generative AI**

**C) What is LLM ?**

**D) What is Embedding ?**

**E) What is a Vector Database ?**

**F) Embedding vs Vector Database**

**G) What is Retriever Augmented Generation (RAG) ?**

**H) What is Langchian ?**

**I) Role of Langchain in RAG**

**J) Prompt Engineering & Fine Tuning**

**V) AI Infrastructure**

**A) What is a GPU ?**

**B) Demo: CPU Vs GPU**

**C) What is RDMA Cluster Network**




# **I) AI Concepts and Workloads - (For Absolute Beginners- Optional)**

# **A) What is AI ?**

Hi folks, welcome back. So we need to start with the one-million-dollar question: What is AI? Right now we are looking to go deep into the certifications for artificial intelligence, but what if we actually don‚Äôt understand what AI really is? How should we start learning these concepts? That‚Äôs why it is very important to first understand what AI actually means. It‚Äôs always good to break down the words. So let‚Äôs break the term Artificial Intelligence and understand the meaning.

First, what do we mean by artificial? Artificial is something that is not natural. When we talk about something natural, we mean something that exists in nature on its own. But when we talk about something artificial, it is man-made. So the first thing to remember is that artificial refers to something created by humans.

Next, what is intelligence? It is a very simple word, but it carries a deep meaning. Intelligence is the ability to acquire and apply knowledge and skills. So when we combine the two terms, we get Artificial Intelligence. This means intelligence that is man-made, not natural. Humans have natural intelligence, but machines and computers have artificial intelligence.

If we look at the complete definition of AI, it is the ability or capability of a computer system to mimic human-like cognitive functions. Now you might ask, what are cognitive functions? Cognitive mainly involves three things: knowing, learning, and understanding. So when we try to make computers smart enough to have human-like cognitive abilities‚Äîwhere computers can understand, learn, and know things‚Äîwe are essentially talking about AI. In short, AI is the ability of a computer system to mimic human-like cognitive functions.

In the next part, let‚Äôs quickly look at a demo of AI. I thought it would be good to give you a quick example of what AI looks like, especially if you are new to this field. So let‚Äôs take a look.

Introducing Phantom‚Äîthe most advanced chessboard in the world. It brings you the infinite possibilities of online chess with the engaging experience of a physical set. Phantom is also the smartest board ever. It allows you to play against any human on Earth remotely, moving the pieces using only your voice. For example, you can simply say, ‚ÄúKnight G4,‚Äù and the board responds. You can also play against its human-like AI, which continuously adapts to your playing level. The pieces can even replay the most famous games in history. Phantom brings back all the little details that make chess great.

This is a perfect example of artificial intelligence. If you noticed, the person controlling the board was simply giving voice commands. In the world of AI, this falls under NLP (Natural Language Processing). You just say ‚Äúmove the knight to G4,‚Äù and based on your speech, the piece moves. The system also includes a human-like AI that adapts to your playing level. The chessboard is so intelligent that it can understand whether you are an expert or a beginner simply based on the type of moves you play. Within the first few moves, it can gauge your level and adjust its own strategy accordingly. A perfect demonstration of AI in action.

# **B) History of AI**

So my dad always says that whenever you have to start with a new subject, you should always begin with its history. That‚Äôs exactly what I‚Äôve done here. I‚Äôve created a complete timeline of artificial intelligence for you, and you‚Äôll actually be surprised to see that artificial intelligence has existed for ages. We‚Äôre talking all the way back to the 1950s. In 1950, a British mathematician named Alan Turing published a groundbreaking paper titled Computing Machinery and Intelligence. This is where the idea of making computers as intelligent as humans truly began. He also introduced the famous Turing Test, a simple method to determine whether a machine can demonstrate human intelligence.

Then we move to 1956, when John McCarthy coined the term Artificial Intelligence at the first-ever AI conference held at Dartmouth College. This is why he is often called the father of artificial intelligence. From there, things began to mature further in the 1960s. In this decade, Frank Rosenblatt built the Mark I Perceptron, the first computer based on a neural network. Don‚Äôt worry if you don‚Äôt know much about neural networks yet‚Äïwe will deep dive into them later. For now, think of neural networks as an attempt by scientists to mimic how the human brain works using neurons. These early systems mostly worked on trial and error: if something went wrong, the algorithm was improved and refined.

Then we jump to the 1980s, when neural networks became more mature. This is when the concept of backpropagation emerged. Backpropagation is a gradient estimation method used to train neural networks by adjusting their internal weights. It was a huge step forward in making machine learning more effective and practical.

In 1997, a major milestone shocked the world. IBM‚Äôs Deep Blue defeated Garry Kasparov, the reigning world chess champion. It was global news because it was the first time a computer had beaten a world champion in chess, proving how far AI had progressed.

Advancing to 2011, AI had another major moment with the game show Jeopardy! Instead of being asked questions, contestants are given clues in the form of answers and must respond with the correct question. IBM‚Äôs Watson competed against champions Ken Jennings and Brad Rutter‚Äîand Watson won. It demonstrated the enormous potential of AI in understanding natural language and processing vast amounts of information.

By 2015, things were heating up even more. The Chinese tech giant Baidu introduced Minerva, a supercomputer that used advanced deep neural networks. Its image identification and categorization abilities exceeded the accuracy of the average human. This was one of the early signals that AI could outperform humans in certain cognitive tasks.

In 2016, another historic moment arrived with the ancient board game Go. Go is a complex strategy game where the objective is to capture more territory than the opponent. DeepMind‚Äôs AlphaGo, powered by deep neural networks, defeated world champion Lee Sedol in a five-game match. Within those five games, AlphaGo proved that AI could master even the most complex strategy-based human games.

Then we arrive at the 2020s, which I always call the true game changer. AI was always there, but what completely changed its face was Generative AI. OpenAI released GPT-3, one of the world‚Äôs most sophisticated language processing models capable of generating human-like text. This is fundamentally different from predictive AI. Generative AI can understand context and create original content‚Äîstories, poems, essays, letters‚Äîjust from a simple prompt. That‚Äôs why I say this era is transformative. From 2020 onwards, you will see rapid evolution in this field, and this is where companies are now putting their R&D budgets.

So with this, I think you now have a solid understanding of the entire AI timeline.

# **C) Benefits of Artificial Intelligence (AI)**

Hello and welcome. After understanding what AI is and exploring the AI timeline, it's now time to look into the benefits of artificial intelligence. Why is AI so important, and why are we learning about it today? The key thing to remember is that AI has been around us for many years, as we saw in the timeline, and its impact continues to grow.

One of the most important benefits is no human error. Humans naturally make mistakes, and many major outages or system failures in the real world have happened due to human error. AI systems, on the other hand, are smart and intelligent machines that follow precise instructions and algorithms. Since the tasks are being performed by computers and not humans, the likelihood of errors significantly reduces.

Another major advantage is 24√ó7 availability. Humans need to eat, sleep, rest, and take breaks in order to function properly, but machines do not. AI systems can work 24 hours a day, 7 days a week, and 365 days a year without getting tired. They are always available and do not require downtime like humans do.

Next is the fact that humans can be biased, whereas machines are not. Humans may be emotionally inclined or unfair due to personal beliefs or preferences. Machines, however, make decisions purely based on the data, algorithms, and models they are trained on, not based on emotions. This leads to more unbiased and consistent decision-making.

AI also enables quicker decision-making. While the human brain is powerful, processing large volumes of data manually can take a lot of time. AI systems excel at parallel processing. You can feed huge amounts of data into a machine, and it can quickly analyze, comprehend, and make decisions much faster than a human could.

AI also helps in reducing risks‚Äîmaybe not ‚Äúno risks,‚Äù but definitely lesser risks. Since machines do not make human errors and do not carry emotional biases, organizations can significantly lower operational and decision-making risks by relying on AI.

One area where AI has shown tremendous progress is healthcare. AI systems are being used to diagnose diseases, predict health outcomes, and even recommend treatment plans. This has elevated the quality of medical assistance and opened new possibilities in patient care, diagnosis, and precision medicine.

Another key benefit is the ability to manage recurring tasks effectively. In many companies, employees are assigned repetitive daily tasks‚Äîlike running the same script every morning at 9 AM. Over time, a human will naturally feel bored, frustrated, or unmotivated because the task is mundane. Machines, however, do not complain, do not get bored, and do not experience fatigue. They can perform repetitive tasks consistently and accurately without any emotional response, making them ideal for automation.

While AI offers many more benefits beyond these, the points covered here represent the most essential advantages you should keep in mind as you continue learning about artificial intelligence.

# **D) Types of AI Workloads**

Now that we‚Äôve already covered the fundamentals of AI, it‚Äôs time to look at the various workloads that exist within artificial intelligence. When we talk about AI workloads, we are basically referring to different categories or types of problems AI is designed to solve. In this section, I‚Äôll explain each workload clearly, and I‚Äôve also referenced small demo videos or examples to help you understand how these workloads work in the real world.

The first workload is Machine Learning. Don‚Äôt worry if you don‚Äôt fully understand machine learning yet‚Äîwe will deep dive into what it is, how it works, and the different types of machine learning later. For now, just remember that machine learning is a branch of artificial intelligence and computer science that uses data and algorithms to imitate the way humans learn and gradually improve accuracy. A simple example is how a child learns to tell the difference between a cat and a dog. When a child is born, they have no idea which is which. But over time, through books, pictures, and guidance from parents, the child learns the features of a cat versus the features of a dog. Machine learning works similarly: we teach a computer model to make predictions and draw conclusions from data, just like humans learn from experience.

A great real-world example is Netflix. When you watch movies on Netflix, the platform recommends new movies or TV shows based on your viewing history. If you watch a lot of thrillers, Netflix learns that and recommends more thrillers. If you prefer romantic comedies, it will show you romcom suggestions. Netflix uses machine learning techniques such as A/B testing to compare algorithms and find out which recommendations bring more viewer satisfaction. Their system continuously learns from user interactions to provide better suggestions.

The next workload is Computer Vision. This is an area of AI that enables computers to identify, detect, and classify objects within images or videos. Until recently, computers could store photos and videos, but they couldn‚Äôt actually understand what objects were present in them. With computer vision, machines can visually interpret the world using cameras, images, and video streams. For example, a system can look at a video and identify people, vehicles, objects, or even track movements. If there was a ball in the scene, the computer could detect that it is a ball. This workload forms the basis for technologies like self-driving cars, surveillance systems, medical imaging, and facial recognition.

Another major workload is Natural Language Processing (NLP). As the name suggests, natural language refers to the way humans communicate‚Äîwhether through spoken or written language. NLP equips computers with the ability to understand human language and respond accordingly. A perfect example is Alexa. You simply talk to Alexa in English, Hindi, or any supported language, and it interprets your speech, processes your request, and responds naturally. Whether you are asking Alexa to solve math problems, make an announcement, or perform a task, you are interacting with an NLP-enabled system that understands your voice commands and replies just like a human would.

Next, we have Generative AI (GenAI), which is one of the biggest technological shifts today. Earlier, AI systems mainly focused on predictive tasks, where the outcome was already defined‚Äîfor example, predicting whether a person has diabetes based on input data. You feed the data, and the system predicts a yes or no outcome. Generative AI, however, goes much further. Instead of only predicting, it has the ability to create original content. This content can be in the form of text, images, code, diagrams, videos, and much more.

For example, suppose you want to surprise your wife on her birthday by writing a poem but you‚Äôre not confident about your writing skills. With generative AI, you simply provide some details‚Äîwhere you met her, what she likes, her positive qualities‚Äîand the system will instantly generate a beautiful poem. Similarly, you can ask it to write Python code, create an image, produce an architectural diagram, or generate a detailed explanation. Tools like ChatGPT demonstrate this perfectly. When you ask ChatGPT to write a 400-word review on gas sensor datasets, it produces the content within seconds. The speed and creativity are remarkable, making generative AI a revolutionary leap forward in how we interact with technology.

With this overview, you should now have a clear understanding of the various AI workloads‚ÄîMachine Learning, Computer Vision, Natural Language Processing, and Generative AI‚Äîand how each plays a major role in shaping the world of artificial intelligence.

# **E) AI vs ML vs DL**

Another key question that comes up when studying AI is understanding the difference between Artificial Intelligence, Machine Learning, and Deep Learning‚Äîoften referred to as AI vs ML vs DL. The best way to visualize this is like an onion with multiple layers. Each layer represents a level of abstraction, with AI at the top, ML in the middle, and DL at the core.

At the topmost layer is Artificial Intelligence (AI). As we‚Äôve discussed, AI is the capability of a computer system to mimic human-like cognitive functions, where cognitive stands for knowing, learning, and understanding. AI is the broadest field and encompasses any technique or system that enables machines to perform tasks that typically require human intelligence.

Beneath AI lies Machine Learning (ML). Machine learning is a subset of AI and a branch of computer science that focuses on using data and algorithms to imitate the way humans learn. ML systems improve their accuracy over time through experience. A simple analogy is a baby learning to distinguish between a cat and a dog. Initially, the baby cannot tell the difference, but through observation, guidance from parents, and books, the child learns to recognize the features of each. A modern example is Netflix, which uses machine learning to recommend movies and shows based on your preferences. The system learns from your interactions and improves its recommendations over time. The key idea here is learning and improving‚Äîmachine learning systems get better as they process more data.

At the innermost layer is Deep Learning (DL). Deep learning is a subset of machine learning that teaches computers to process data in ways inspired by the human brain, particularly using neural networks. It enables the computation of multi-layer neural networks, making complex tasks feasible. While we will explore neural networks in more detail later, it‚Äôs important to understand that deep learning forms the foundation for advanced technologies like driverless cars and generative AI systems. Deep learning allows machines to process vast amounts of unstructured data, recognize patterns, and make complex decisions autonomously.

In summary, the relationship between AI, ML, and DL can be visualized as layers: AI is the topmost layer, encompassing all intelligent behavior; ML sits beneath AI, focusing on systems that learn and improve; and DL forms the core, enabling highly complex tasks through neural networks inspired by the human brain. Understanding this layered mechanism helps clarify the distinctions and connections between these three critical areas of artificial intelligence.

# **II) Machine Learning Foundations (For Absolute Beginners-Optional)**

# **A) Machine Learning - Real World Example**

Okay, so let's take a look at a real-world example of machine learning. So it's just people who are actually working on machine learning at Netflix day in and day out, and they'd be sharing their experiences. So for the moment, just try to have a look. Don't worry. We'll be going into the theoretical part of machine learning just after this video. At Netflix, we have over 120 million members. They span the whole globe across 190 different countries, as well as a diversity of titles, content, comedies, and dramas. Machine learning is deeply intertwined with all aspects of Netflix's business‚Äîhow we compose our catalog of content, how we produce our content, how we encode it, how we stream it. We use machine learning to help marketing efforts and advertising efforts. Then we also use machine learning in our content acquisition effort. Okay, so this was just a quick snippet or a video on machine learning in the real world. In the next video, we'll start learning about some of the key terminologies of machine learning because once you get a good understanding of the key terminologies, then you can have a good hold of the machine learning fundamentals. Thanks for watching.

# **B) Machine Learning - Key Terminologies**

Hi folks. Welcome back. So now it's time to go into the theoretical definitions and the different terminologies for machine learning. Before we actually do a deep dive into machine learning, I thought it would be better to give you a good understanding of different terminologies. This is very important because whenever you are reading about machine learning, artificial intelligence, or when you're talking to AI professionals or data scientists, these are the kind of words they will keep on saying. If you don't have a good understanding of these words or terminologies, you'll really struggle in the field of machine learning or AI. So let's take a look at these fundamental concepts. The very first one is an algorithm. What is an algorithm? A general definition of an algorithm, when we studied math or physics, is that an algorithm always refers to a set of rules. 

In machine learning, an algorithm refers to a set of rules and statistical techniques. Now you might remember what statistics is. A clear definition of statistics is that it is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data. In other words, you have a lot of data, and from that data, you need to draw conclusions. So algorithm is a set of rules and statistical techniques. A good example of this is the decision tree algorithm. A perfect example of that is Gmail. When Gmail came, it never had a concept of filters. After some time, they came up with the concept of filters to classify emails as spam or not spam. They look at your emails and, based on a certain set of rules, they tell you which emails are spam and which are not. They do this based on features‚Äînow called features or artifacts in machine learning‚Äîsuch as who the sender is, frequency of certain words, or specific phrases like "you have won a lottery" or "there is money in your account." These features help the algorithm decide whether an email is spam or not. This is your algorithm. The next concept is a model. A model is what an algorithm creates. 

Always remember, a model is derived from an algorithm. A model is essentially the learned representation of data. It is a program that can find patterns and make decisions. For example, a dataset of house features and prices fed into a regression algorithm can create a model that predicts the price of new houses based on their features. If you feed new data to the model, it can predict price ranges for houses. So a model is an algorithm that has been trained on data, and it helps find patterns to make decisions. Now comes the concept of training. You always train your model, similar to training a child to make decisions. Training is the process of presenting data to a machine learning algorithm to create a model. The algorithm uses training data to learn. For example, a neural network shown thousands of pictures of cats and dogs can learn to identify whether a new image is a cat or a dog based on features like ears, eyes, and other patterns. Then, there's the concept of labels. A label is the output you want the model to predict. For example, in healthcare, the label could be whether a tumor is malignant or benign. Malignant means dangerous, while benign means it is not a major issue. To summarize, algorithm is a set of rules, a model is what an algorithm creates after being trained on data, training is the process of presenting data to an algorithm to create a model, and labels are the outputs the model is trying to predict. Thanks for watching.

# **C) What is Machine Learning ?**

Hello and welcome. So the time has finally come to talk about machine learning. Before we go into the details of what machine learning is, it‚Äôs important to understand how humans learn. Just think about it‚Äîwhen a baby is born, can the baby distinguish between a cat, a dog, and a horse? No. A baby learns by observing different features. For example, if an animal has long ears and a black nose, it might be a dog. If it has small size and blue eyes, it could be a cat. Humans learn primarily by examples, diagrams, and comparisons. Similarly, we train machines to learn using data examples.

For instance, if we feed thousands of labeled photographs of dogs, cats, and horses to a computer, it can identify patterns based on features like ear length, nose shape, and eye color. These patterns are used to train an algorithm, and once the model is built based on that algorithm, it can make predictions on new data. This model can then classify a new image as a cat, dog, or horse based on its features.

Theoretically, machine learning is a branch of artificial intelligence. As discussed earlier in the AI-ML-DL ‚Äúonion‚Äù analogy, machine learning (ML) is a subset of artificial intelligence (AI). ML focuses on the use of data and algorithms. If I were to define the heart and soul of machine learning, it would be data. Machine learning helps machines imitate how humans learn, gradually improving their accuracy. Computers apply statistical learning techniques to automatically identify patterns in data. Statistics itself is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data.

Data contains patterns, and algorithms are used to find these patterns. Using our cat and dog example, features in the data are analyzed to find patterns, which are then used to train the algorithm. Once trained, a model is created. This model can recognize patterns in new data. Data is typically divided into training data, which is used to train the model, and testing data, which is used to evaluate its accuracy. The model is then used to make predictions on new, unseen data.

As machine learning progresses, increased data and experience improve the accuracy of the results, similar to how humans learn gradually. Machine learning algorithms improve as they process more data, updating their parameters to learn over time. There are three main approaches to machine learning: supervised learning, unsupervised learning, and reinforcement learning.

Supervised learning involves labeled data. For example, in the cat and dog dataset, features are labeled to indicate whether the animal is a cat or a dog. Unsupervised learning deals with unlabeled data and helps identify patterns or structure in such data. Reinforcement learning is based on rewards and penalties. Similar to how a teacher rewards a student for correct actions and penalizes mistakes, machines learn by maximizing rewards and minimizing penalties over time.

Data is the foundation of machine learning. Both the quality and quantity of data are critical. Garbage data will result in poor models, while high-quality data improves accuracy. Likewise, larger datasets help models better understand patterns and improve predictions. For example, feeding 100 rows of data will train a model to some extent, but 10,000 rows will give the model a much better understanding of the patterns.

Algorithms in machine learning are sets of rules, often based on statistical or mathematical techniques. They are tailored to specific types of data and learning tasks. As seen in real-world examples like Netflix, machine learning is applied across diverse fields, including healthcare, finance (e.g., fraud detection), autonomous vehicles, and tasks like prediction, classification, clustering, and deployment. Once trained, models are deployed in real-world applications to provide insights, automate tasks, or enhance decision-making.

In conclusion, machine learning is essentially about teaching machines to learn like humans. By feeding data, finding patterns, and building models, machines can make predictions and decisions. Data-centric machine learning relies heavily on the quality and quantity of data. Always remember‚Äîthink of machine learning as training a baby: the better and more comprehensive the training data, the better the machine learns.

# **D) Types of Machine Learning**

Okay, so now we have got a very good understanding of what machine learning is. We have seen some real-world examples of machine learning, and we also discussed the workflow of machine learning. Now, it is important to understand the various types of machine learning algorithms. In this section, we will do a deep dive into each of these types.

First, we will learn about supervised machine learning. Supervised learning is where the model training is done using labeled data. Don‚Äôt worry‚Äîwe will explore this in detail, and we will understand concepts like classification and regression.

Next, we will talk about unsupervised learning. Unsupervised learning is called ‚Äúunsupervised‚Äù because it uses unlabeled data. In unsupervised learning, we have the concept of clustering. Each of these algorithms or models addresses different use cases, which we will also discuss in detail later.

Then comes reinforcement learning. As discussed earlier, reinforcement learning works based on the reward and punishment method. Just like a baby or a student learns by receiving rewards for correct actions and penalties for mistakes, reinforcement learning works on a feedback loop. The model takes actions in an environment, receives feedback and state updates, and gradually learns to make better decisions.

So, with this quick overview, we can summarize that there are mainly three types of machine learning algorithms: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning includes classification and regression, unsupervised learning mainly focuses on clustering, and reinforcement learning relies on feedback from the environment.

In the next video, we will go deeper into supervised learning. See you in the video. Thanks for watching.

# **E) What is Supervised Machine Learning ?**

Hello and welcome. It's time to take a look into supervised machine learning. Whenever we learn a new subject, it helps to look at its root word. In this case, the root word is ‚Äúsupervisor.‚Äù A supervisor is a person in charge of a group, ensuring that work is done correctly, accurately, and according to the rules.

The term supervised machine learning comes from the fact that the algorithm is trained using labeled data. Labeled datasets are annotated with meaningful tags or labels that classify the outcome. For example, in our earlier cat and dog example, we gave features such as nose type, ear type, and ear length, and labeled the animal as either a cat or a dog. In this case, the labeled dataset acts as the supervisor‚Äîit guides the algorithm in learning the correct output.

Supervised learning involves training a model on a dataset that includes both input data and the corresponding correct output. A classic example is the Iris flower dataset, which contains various types of Iris flowers, such as Setosa, Versicolor, and Virginica. Each flower has features like sepal length, sepal width, petal length, and petal width. The input data consists of these features, while the output data is the flower‚Äôs class. By providing thousands of such examples, the model can learn patterns in the data and make accurate predictions for new inputs.

The training dataset must be labeled, meaning each example is paired with the correct answer. For instance, if you provide sepal and petal measurements along with the corresponding flower type, the model learns to map the input features to the correct class. This mapping allows the model to make predictions on unseen data.

There are different types of algorithms in supervised machine learning, including linear regression, logistic regression, support vector machines (SVMs), decision trees, and neural networks (the latter being explored further in deep learning). The training process is straightforward: the model learns a function that maps inputs to the desired outputs and makes predictions based on that function. For example, if you feed a model new data such as sepal length 5.9, sepal width 3.0, petal length 5.0, and petal width 1.8, it should predict that the flower is a Virginica.

One risk in supervised machine learning is overfitting, where the model performs very well on training data but gives inaccurate results for unseen data. Overfitting occurs when the model learns patterns that are too specific to the training set, reducing its ability to generalize.

Supervised machine learning has a wide range of applications. Examples include image recognition, where models identify cats and dogs in images; speech recognition, such as voice-based banking systems where the model identifies a user‚Äôs voice as a password; medical diagnosis, where models help identify diseases; spam detection, which classifies emails as spam or not spam; and stock price prediction, which forecasts continuous values.

In supervised learning, there are two main types: classification, which sorts items into categories, and regression, which predicts continuous values. For example, classification can label an email as spam or ham, while regression can predict a house price based on features. We will explore these types in more detail in the next videos, looking at the differences between classification and regression.

# **F) What is Classification in SML ?**

So there are two types of machine learning, or supervised machine learning per se. One is classification and the other is regression. As I always say, it‚Äôs important to look at the root word. In classification, the root word is classify. You may ask, what does classify mean? It means to arrange a group of people or things into classes or categories. It‚Äôs as simple as that. Essentially, you have many things, and you are just grouping them.

For example, consider I have a brinjal, a broccoli, a carrot, and some burgers. You want to categorize them. If you pass this through a classification model or machine learning model, it can classify that all the vegetables fall under the category ‚Äúvegetables,‚Äù and the burgers fall under ‚Äúgroceries.‚Äù If a company wants to do analytics to see what customers are buying, and categorize how much is vegetables and how much is groceries, this is exactly what classification does.

In terms of a formal definition, classification involves assigning category labels to new observations based on past observations and their labels. In supervised machine learning, we have labeled data. The aim of classification is to assign category labels, and these labels are known when you input your data. Classification can be binary, meaning two classes, like determining whether an email is spam or not (non-spam is also called ham). A lot of emails are fed into the classifier, and based on features or algorithm logic, it identifies which ones are spam and which ones are ham, sending them to the appropriate folder.

Classification can also be multi-class, such as classifying types of fruits. There are multiple fruit categories, and the model identifies which category each fruit belongs to. Common algorithms used for classification include Decision Trees, Random Forests, Logistic Regression, Support Vector Machines, and Neural Networks. While you don‚Äôt need to fully understand each yet, it‚Äôs important to know their names.

Feature selection is crucial in classification. Choosing the right features or inputs determines the effectiveness of the model. For example, in the healthcare sector, detecting prostate cancer requires feeding MRI images to the model. The feature design determines whether the image contains a lesion or not. A lesion indicates a damaged area or possible cancer. The machine, using these features, classifies whether the image falls under the prostate cancer category or another category. This is far more efficient than manually analyzing thousands of images. Selecting the right features is critical because if features are poorly chosen, the model might miss identifying a patient with cancer.

As a supervised machine learning technique, classification requires labeled data. Each instance in the training data is tagged with the correct class. When test data is introduced, the model should predict the appropriate category based on what it has learned from training.

Another important concept is overfitting and underfitting. Overfitting occurs when a machine learning model performs very accurately on training data but fails to generalize to new or test data. It is considered undesirable. Balancing model complexity to prevent overfitting and underfitting is essential. Testing helps adjust and tweak the model so that its performance on new data mirrors its performance on training data.

Classification has many applications. Email spam detection is one example. Image recognition is another; for instance, analyzing MRI prostate images to detect cancer. It‚Äôs also used in medical diagnosis and sentiment analysis, and it can even assist in real-time decision-making. In banking, classification is used for fraud detection. For example, based on transaction features and patterns, a system can classify transactions as normal or potentially fraudulent.

To summarize, supervised machine learning classification is straightforward. It involves classifying or adding category labels‚Äîessentially arranging a group of people or things into classes or categories. By understanding features and carefully training models, classification can be applied to various sectors effectively, from healthcare to banking to everyday analytics.

# **G) What is Regression in SML ?**

Hi folks. Welcome back. In the previous video, we discussed supervised machine learning classification. Now, we will talk about regression. You might ask, what is regression? Regression is a statistical technique that relates a dependent variable to an independent variable. You might also ask, what is a dependent variable and what is an independent variable? Let me explain with a very simple example.

If we have an equation like ùë¶=2ùë•y=2x, something most of us learned in algebra, here ùë• x is the independent variable and ùë¶ ;ùë• x. For example, if ùë• = 2 x=2, then ùë¶ = 4 y=4. 

y dependent? Because its value changes as we change y=6. The independent variable is plotted on the x-axis, and the dependent variable is plotted on the y-axis, usually in 
a scatter graph showing various data points. Regression analysis predicts a continuous output, which is the dependent variable. Essentially, we try to determine the value of ùë¶ y based on one or more predictors (independent variables). For instance, in house price prediction, the general assumption is that the larger the house, the higher the price. So if a house is 2500 square feet, its price would likely be in a high range. If your test data has a house of 1250 square feet, regression helps predict the dependent variable, i.e., the price, which might be around 220 (thousand or lakhs, depending on units). Keep in mind, this is just a prediction, not the actual price.

There are two types of regression: linear regression and multiple regression. Linear regression is the simplest form and assumes a linear relationship between the input and output variables. For example, predicting house price based on its size is a linear regression problem. In linear regression, there is only one independent variable.

In contrast, multiple regression involves two or more independent variables. For example, predicting a car's price could involve features such as age, mileage, brand, engine size, and more. By including multiple independent variables, we can make more accurate predictions.

The line of regression is the line used to predict the value of ùë¶ y for a given ùë• x. This can be thought of as a model representation, like ùë¶=2ùë• y=2x in a simple example, but regression algorithms can compute this line based on training data.

Regression has many applications. In finance, it can be used for stock price prediction based on historical data. In medical diagnosis, it helps predict outcomes or measurements. In retail, regression can forecast sales based on historical trends and market behavior.

As with classification, regression models can also suffer from overfitting and underfitting. Overfitting occurs when a model performs very well on training data but poorly on unseen data, such as predicting future stock prices. A complex model may fit historical data perfectly but fail to generalize to new data. Therefore, it is essential to tune the model so it performs well on both training and test data. Most machine learning algorithms require this balance.

To clarify the difference between regression and classification: if you are predicting whether it will be hot or cold tomorrow, you are categorizing the outcome‚Äîthis is classification. If you are predicting the exact temperature tomorrow, you are predicting a continuous output‚Äîthis is regression. Regression analysis is all about predicting the value of a dependent variable based on one or more independent variables.

# **H) What is Unsupervised Machine Learning ?**

After gaining a solid understanding of supervised machine learning, it‚Äôs now time to explore unsupervised machine learning. In unsupervised learning, there are mainly two types: clustering and association.

The golden rule to remember is that supervised machine learning works on labeled data. For example, if you feed a lot of pictures of cats and dogs, you label each picture as either a cat or a dog. This labeled data acts as the supervisor, guiding the machine learning model. In contrast, unsupervised machine learning deals with unlabeled data. Here, you feed the algorithm data without providing any output labels. For instance, you insert a lot of pictures, but you don‚Äôt tell the machine which ones are cats or dogs. This is the core of unsupervised learning.

By definition, unsupervised learning involves analyzing and clustering unlabeled data sets. You might ask, what is clustering? Clustering is essentially grouping data. The algorithm groups similar data points together‚Äîforming clusters. For example, cluster one contains data points that are similar, and cluster two contains another type of data points. Clustering allows the discovery of hidden patterns in the data. The goal of unsupervised machine learning, like supervised learning, is to understand patterns and make predictions, but here it does so without guidance from labeled data.

Clustering is the most common unsupervised learning technique. It groups data points into clusters where items in the same cluster are more similar to each other than to items in other clusters. For instance, if you feed different pictures into the algorithm, it might place all the cats in one cluster and all the dogs in another. This technique is widely used in market segmentation, where customers are grouped based on purchasing behavior. Companies can then target specific clusters with tailored marketing strategies.

Another important unsupervised technique is association. Association identifies sets of items that frequently occur together. A perfect example is online learning platforms like Udemy. When you purchase an Oracle Cloud course, the platform may recommend three other courses frequently bought together by other users. This helps learners discover relevant courses and often comes with bundled discounts. Similarly, in retail, association rules help find products often bought together, optimizing store layouts or promoting cross-sales.

Anomaly detection is another important aspect of unsupervised learning. It is used to identify unusual patterns, such as detecting hackers intruding into a network or spotting fraudulent transactions. By examining transaction patterns, the algorithm can flag behaviors that deviate significantly from the majority of data, enabling fraud detection in banking or cybersecurity applications.

Unsupervised learning has broad applications, including bioinformatics, image recognition, speech recognition, and recommender systems. Platforms like Netflix and Spotify use unsupervised learning to group users with similar interests. For example, if a user frequently watches thriller movies or takes Oracle Cloud courses, the system recommends similar content without needing labeled data for each user.

However, unsupervised learning comes with challenges. Compared to supervised learning, it is more difficult because there is no labeled data. One of the biggest challenges is determining the correct number of clusters in a dataset without predefined categories. While simple examples like cats and dogs are straightforward, real-world datasets often have many varieties, making it complex to identify clusters accurately. Data scientists must carefully tune their models to handle these complexities.

In summary, unsupervised machine learning focuses on discovering patterns in unlabeled data. Its two main types are clustering, which groups similar data points, and association, which identifies items that frequently occur together. Despite the challenges, unsupervised learning is a powerful tool for understanding hidden patterns and making informed decisions.

# **I) What is Re-inforcement Machine Learning**

In the previous videos, we learned about supervised and unsupervised machine learning. Now, it‚Äôs time to explore the third type of machine learning, called reinforcement learning. As always, it helps to look at the root word. Reinforcement means to further strengthen or to give additional strength. In everyday language, we often hear about reinforcement in the context of the army‚Äîsending a reinforcement army to strengthen a deployed force. Similarly, in machine learning, reinforcement means strengthening a model or moving toward more accurate predictions. The ultimate goal is to achieve maximum accuracy, and this is achieved through a process of continuous feedback and improvement.

The core concept of reinforcement learning involves an AI agent. The agent learns to make decisions by performing actions and receiving feedback. Feedback comes in the form of rewards and penalties. You can think of it like a student in a classroom. If the student does well, they receive a reward. If they make a mistake, they may face a penalty. Similarly, the AI agent performs actions, observes the results, and adjusts its strategy to maximize rewards and minimize penalties.

In reinforcement learning, the agent interacts with its environment. At each time step, the agent receives the state of the environment, takes an action, and gets feedback in the form of a reward or penalty. For example, in a game of chess, the environment is the chessboard, the actions are the moves (like moving a king, queen, or knight), and the feedback is whether that move improves the agent‚Äôs position (reward) or worsens it (penalty). The agent is the heart of reinforcement learning, learning continuously from its interactions with the environment.

The learning process involves selecting the best action given a state to maximize cumulative rewards over time. Two critical concepts are exploration and exploitation. Exploration refers to trying new actions to discover potential strategies, while exploitation means leveraging existing knowledge to make the best decision based on what the agent already knows. Balancing exploration and exploitation is a key challenge, as relying too much on either can reduce the model‚Äôs effectiveness.

Reinforcement learning has several important applications. Game playing, such as chess or Go, is a classic example. Robotics and autonomous vehicles also rely heavily on reinforcement learning. For instance, companies like Waymo constantly improve their autonomous vehicle algorithms by learning from real-world driving experiences. Accidents or mistakes act as penalties, prompting the model to improve and adapt.

However, reinforcement learning comes with challenges. It requires large amounts of high-quality data, balancing exploration and exploitation, and handling environments with high variability or uncertainty. In autonomous driving, for example, unexpected events‚Äîsuch as a child running onto the road or a cyclist falling‚Äîcreate an unpredictable environment. The AI agent must learn to adapt safely and efficiently in such scenarios.

In summary, reinforcement learning focuses on strengthening models through trial, feedback, and improvement. The agent interacts with the environment, takes actions, and receives feedback in the form of rewards or penalties to optimize performance over time. It is a powerful approach for complex decision-making tasks where continuous learning and adaptation are critical.

# **J) What is a Jupyter Notebook**

This is a very important concept: Jupyter Notebooks. I want to give you a clear understanding of what a Jupyter Notebook is, because if you‚Äôre working in machine learning, data science, or artificial intelligence, a solid grasp of Jupyter Notebooks is essential.

A Jupyter Notebook provides an interactive environment for writing and running code in various programming languages, most notably Python. The beauty of a Jupyter Notebook is that you can write your code and see the output in the same interface.

If you look at a notebook, you‚Äôll notice it is organized into cells. Each cell can contain code, and you can execute it independently. For example, you can run one code cell, see its output, and then move on to the next cell. This live code execution makes Jupyter Notebooks very convenient for experimenting and iterating on your code.

Although Jupyter supports around 40 programming languages (including R, Julia, and Scala), Python is the primary language used for data science and machine learning.

Another advantage of Jupyter Notebooks is the ability to include rich text elements. You can add markdown text, equations, images, and links, which allows you to document your code comprehensively alongside its explanations. This makes notebooks highly readable and useful for sharing and teaching.

Jupyter Notebooks also have excellent integration with data visualization libraries. For instance, Python‚Äôs matplotlib allows you to create static, animated, and interactive visualizations. Other libraries like Plotly and Bokeh can be used for interactive graphs. When you run a code cell, the output‚Äîwhether text, table, or graph‚Äîappears directly below it, making the experience highly interactive.

You can also share your notebooks easily. They can be exported to different formats, such as HTML, PDF, or even slides, which makes them ideal for presentations and collaboration. This is one reason Jupyter Notebooks are widely popular in both academia and research. They are used extensively in universities and research projects for teaching programming, computational thinking, and data analysis.

Furthermore, Jupyter Notebooks integrate well with several data science tools, such as Anaconda. This integration allows you to perform tasks like data cleaning, statistical modeling, machine learning, and more, all within a unified environment. Even Oracle Machine Learning tools support Jupyter Notebooks, enabling seamless use in enterprise environments.

So, that‚Äôs why understanding Jupyter Notebooks is essential. In the next video, we‚Äôll do a demo showing how to use Anaconda to run Jupyter Notebooks. From there, we‚Äôll move into machine learning demos and start practical work.

# **K) Demo: Install Anaconda**

The time has finally come to learn something practically. Until now, we have been focusing a lot on the theoretical aspects of machine learning, but now it‚Äôs time to understand it in a practical manner. For this, it‚Äôs very important to have a good understanding of Anaconda. You might ask, what is Anaconda?

Anaconda is an open-source distribution of Python and R, designed specifically for data science. Its main purpose is to simplify package management and deployment. The package versions in Anaconda are managed by a system called Conda, which analyzes your current environment before executing an installation. This ensures that existing frameworks or packages are not disrupted. The Anaconda distribution comes with over 250 packages pre-installed, making it highly convenient for data science, machine learning, and AI work. Therefore, anyone looking to work in these fields should definitely understand what Anaconda is and how it works.

Getting started with Anaconda is simple. By going to Anaconda.com, you can select the free download option. Anaconda automatically detects your system and provides the correct installation file. There are installers available for Windows, Mac, and Linux. For Mac users, there are two options depending on the processor type, either Intel or M1/M2/M3. Anaconda is open-source, user-friendly, and trusted by many companies. Behind the scenes, it uses Conda to run all your code efficiently and manage the environment seamlessly.

One important part of Anaconda is the Anaconda Navigator, which is a graphical interface that allows you to manage, integrate, and run applications, packages, and environments without needing to use the command line. It comes pre-built with several important packages used for data exploration, transformation, visualization, machine learning, and more. These packages make it easy to perform a wide variety of tasks in a single environment.

Once Anaconda is installed, you can open the Anaconda Navigator, which allows you to launch applications such as Jupyter Notebooks. Jupyter Notebooks provide an interactive environment where you can write and run Python code. Each notebook consists of cells that can be executed independently. You write your code in a cell, run it, and the output appears directly below the cell. This live interaction makes it easier to experiment with code and immediately see results.

The beauty of Jupyter Notebook is that it also supports the integration of various Python libraries, enabling the creation of graphs, visualizations, and interactive outputs within the same environment. This makes it an essential tool for data science and machine learning tasks. By combining Anaconda and Jupyter Notebooks, you have a complete setup for writing, testing, and visualizing code in a highly interactive way.

In this video, we provided an overview of how to use Python in Jupyter Notebooks through Anaconda. In the next session, we will work with a real dataset, specifically the Iris dataset, to see machine learning in action. This will help bridge the gap between theory and practical application in a hands-on way.

# **L) Demo: Understanding the IRIS Dataset**

Now I‚Äôd like to introduce you to the Iris dataset. You might recall from one of our previous lectures on supervised machine learning, where we discussed labeled datasets. The Iris dataset is a classic example of a labeled dataset, and it has a long history in the field of machine learning. In fact, it was first used in R.A. Fisher‚Äôs 1936 paper, which shows just how foundational it is. The dataset is also available on the UCI Machine Learning Repository.

So what exactly is the Iris dataset? Essentially, it contains several features related to iris flowers. These features include sepal length in centimeters, sepal width, petal length, and petal width. The dataset also includes labels that indicate the species of the iris flower. The main species in the dataset are Iris setosa, Iris versicolor, and Iris virginica. There are 50 entries for each species, giving us a total of 150 rows. Because it is a labeled dataset, each row not only contains the features but also the corresponding species category, which makes it ideal for supervised learning tasks.

Our first step is to understand the dataset before we start building models. You can easily download the Iris dataset from the internet, or I will also provide it in the course resources as a CSV file named iris.csv. Once you have the file, you can begin exploring it using Python in a Jupyter Notebook. To start, we typically import the pandas library and read the CSV file into a variable. This allows us to view and manipulate the data easily. For example, using the head function, you can quickly see the top five rows of the dataset, which show the sepal length, sepal width, petal length, petal width, and the species of each flower. This is essentially your training data, which will be used to train your machine learning model.

Next, you might want to understand the overall structure of the dataset. By checking the shape of the data, you can see that there are 150 rows and six columns. This gives you a sense of the size and the number of features in the dataset. Sometimes, you may only be interested in specific columns. For instance, if you want to display just the ID and species columns for the first ten records, you can filter the dataset accordingly. This allows you to focus on only the relevant information while exploring the dataset.

This overview provides a solid understanding of the Iris dataset and sets the stage for practical machine learning. In the next video, we will take the Iris dataset and create our own machine learning model. We will train the model using this dataset and then make predictions to see how it performs. Things are about to get interesting, so keep watching.

# **M) Demo: Creating & Training your ML Model**

The Iris dataset is one of the most famous and foundational datasets in machine learning. It dates back to R.A. Fisher‚Äôs classic 1936 paper and is still widely used for teaching classification techniques. Now that we have studied supervised machine learning theoretically, this is the perfect time to start applying those concepts practically. In this exercise, we will load the Iris dataset, split it into features and labels, create a machine learning model, train it, and finally perform predictions using the trained model.

We begin by splitting the dataset into X and Y. As explained earlier, the X-axis (or the independent variables) contains all the features, while the Y-axis (or the dependent variable) contains the label we want to predict. In the Iris dataset, we drop the ID and species columns from the input features because species is our output label, and ID is irrelevant for prediction. This leaves us with four key features: sepal length, sepal width, petal length, and petal width, which form our X. The Y variable contains only the species column, which represents the type of Iris flower.

To verify this separation, we print X.head() and Y.head() separately. Printing the X values shows the four numeric feature columns, while printing Y shows the species names such as Iris-setosa, Iris-versicolor, and Iris-virginica. This simple step ensures that our data is correctly divided into inputs and outputs before we move into modeling.

Next, we create our machine learning model. We use the Logistic Regression algorithm from sklearn.linear_model. Logistic Regression is a classification algorithm and fits perfectly for this dataset. We initialize the model with default settings and store it in a variable, for example, ml_model. After initialization, we train the model using ml_model.fit(X, Y), where X contains the features and Y contains the corresponding class labels. This is where the model learns patterns from the training data.

Once the model is trained, we move on to testing its predictive ability. Testing is always done on new data (data the model hasn‚Äôt seen during training). We create a test input by manually specifying sepal length, sepal width, petal length, and petal width. This simulates a real-world scenario where we would provide measurements of a new flower and ask the model to classify it. We pass this data to the model using ml_model.predict() and store the result as predictions.

Finally, we print the model‚Äôs prediction. Using only the four numerical features, the trained model is now intelligent enough to tell us which species the new flower most likely belongs to. For example, it may correctly classify the flower as Iris-setosa. This end-to-end example perfectly demonstrates how machine learning can take raw numerical inputs and produce meaningful predictions based on learned patterns.

# **III) Deep Learning Foundations (For Absolute Beginners- Optional)**

# **A) What is Deep Learning ?**

Deep learning is an advanced branch of artificial intelligence and machine learning, and as the name suggests, it takes us much deeper into understanding how machines can learn. To grasp deep learning properly, it helps to first think about the human brain. Our brain is made up of millions of interconnected nerve cells called neurons. These neurons send signals throughout the body and enable us to perform everything from breathing and walking to speaking and thinking. The key idea behind deep learning is to mimic the way these biological neurons process information and communicate with each other.

In deep learning, this concept is implemented through artificial neural networks. These are computer algorithms inspired by the structure and functioning of the human brain. Just like real neurons are interconnected, artificial neurons‚Äîor nodes‚Äîare also connected across multiple layers. A human brain contains millions of connections, and similarly, complex deep learning models contain many layers of artificial neurons working together to process data and make decisions. The resemblance between a biological neural network and an artificial neural network is the foundational idea behind deep learning.

One of the major advantages of deep learning is that it reduces the need for manual feature extraction. In traditional machine learning, humans often identify which features of the data are important‚Äîfor example, labeling images as cat or dog or specifying sepal length and sepal width in the Iris dataset. In deep learning, this dependency is significantly reduced. The neural network automatically learns the important features from raw data, combining feature extraction and classification into a single unified process. Although human involvement does not disappear completely, deep learning greatly minimizes how much manual intervention is needed.

Deep learning models consist of multiple layers, and this is exactly why the term ‚Äúdeep‚Äù is used‚Äîbecause data passes through many layers of transformations. Each layer learns increasingly complex patterns. These layers of interconnected nodes work together to recognize patterns that would be difficult to identify manually. This structure allows deep learning to excel in handling unstructured data, such as images, audio, and text‚Äîdata that doesn‚Äôt follow a fixed format. For example, deep learning can recognize faces in images using Convolutional Neural Networks (CNNs) or understand human language through NLP models.

Because deep learning relies heavily on large neural networks, it requires massive amounts of labeled data to train. Unlike machine learning models that might work with thousands of rows, deep learning often needs millions of examples to achieve high accuracy. With such large datasets comes the need for high computational power. This is why GPUs (Graphical Processing Units) are commonly used. GPUs are excellent at processing large volumes of data in parallel, which makes them perfect for training deep learning models. Later, when we discuss AI infrastructure, we will dive deeper into GPUs and how they accelerate training.

Deep learning is now used in a wide variety of applications. These include autonomous driving systems, medical diagnosis, natural language processing, speech recognition, fraud detection, and much more. Much of modern AI‚Äîlike chatbots, virtual assistants, and image-based search‚Äîrelies heavily on deep learning models.

Finally, deep learning is supported by powerful tools and frameworks such as TensorFlow, PyTorch, and Keras. Keras, in particular, is a user-friendly deep learning library in Python that makes it easier to build and train neural networks. In upcoming lectures, we will explore Keras in detail with hands-on demonstrations to give you a deeper understanding of how deep learning models are built.

# **B) What is a Neural Network ?**

After understanding what deep learning is, the next important step is to explore what a neural network actually means. Before we dive deeper, let‚Äôs quickly recap what we learned earlier: deep learning is a machine learning technique that teaches computers to process information in a way inspired by the human brain. Our brain contains millions of interconnected neurons, and these neurons work together across many layers to help us perform everyday tasks‚Äîeating, drinking, thinking, deciding, and so on. These biological neurons form complex networks, and this very idea forms the heart of deep learning.

The core mechanism that powers deep learning is the neural network. In biology, neurons are the building blocks of the nervous system, but in artificial intelligence we work with artificial neurons. So you may ask: what exactly is a neural network? A neural network is simply a machine learning model designed to make decisions in a way that resembles how the human brain works. Inspired by the brain‚Äôs structure, scientists created artificial neurons and connected them together so that these networks can learn patterns, identify features, and make informed decisions just as biological neurons do.

A neural network is built from layers of interconnected nodes. Just like biological neurons are connected to one another, artificial neurons also form connections through which information flows. The structure of a typical neural network consists of three major components: the input layer, where the data enters; the output layer, where predictions are produced; and the hidden layers, which sit in between. These hidden layers can be many in number, and they are the reason neural networks become ‚Äúdeep,‚Äù giving rise to the term deep learning. These layers enable the network to learn complex relationships in data.

Each artificial neuron in a neural network performs a simple mathematical operation. It receives inputs, multiplies them with weights, adds a bias, and then applies a nonlinear function known as an activation function. This activation function decides whether the neuron should activate or not. To understand this better, imagine feeding an image of a circle to a neural network. Suppose the image is 28√ó28 pixels. That means we have 784 pixel values, and each pixel is sent to a corresponding neuron in the input layer. These neurons are connected to neurons in the next layer through channels, and each connection carries a weight‚Äîfor example, 0.8 or 0.2. These weights determine the strength or importance of the signals being passed along.

The process where information moves from the input layer to the output layer is called forward propagation. The network processes the inputs layer by layer, and finally makes a prediction‚Äîperhaps it incorrectly predicts the circle as a square. When the prediction is wrong, the neural network adjusts itself using a process called backward propagation. During this process, the weights are recalibrated to reduce the error‚Äîfor example, a weight of 0.8 might be updated to 0.6. This adjustment continues gradually until the network becomes accurate. This is how neural networks ‚Äúlearn by example,‚Äù just like humans learn from repeated observations.

Neural networks also use biases, which are extra parameters added to neurons to help them make better decisions. Together, weights and biases help the network determine the final output through the activation function. As the training progresses, these values keep updating, improving the model‚Äôs performance.

There are several types of neural networks, each used for different applications. Feedforward Neural Networks are the simplest form, where information moves only in one direction. Convolutional Neural Networks (CNNs) are widely used for image recognition tasks, such as identifying objects, faces, and scenes in images. Recurrent Neural Networks (RNNs) are commonly used for speech recognition and sequence-based tasks‚Äîthis is how virtual assistants like Siri or Alexa recognize your voice and respond appropriately.

Neural networks also power technologies we use daily. Predictive text on smartphones uses neural networks to guess the next word you might type‚Äîfor example, after writing ‚ÄúHello, how are,‚Äù the model predicts ‚Äúyou.‚Äù Beyond these examples, neural networks play key roles in self-driving cars (such as Waymo), medical diagnosis, financial forecasting, stock market trading, and many other real-world applications. While the examples seem familiar across machine learning, deep learning, and neural networks, what improves significantly is the accuracy, power, and ability to extract complex patterns automatically.

Ultimately, the entire idea behind neural networks‚Äîand deep learning as a whole‚Äîcomes back to two fundamental goals: identifying patterns and making predictions. That is what every model aims to do. With this, we complete our understanding of what a neural network is.

# **C) Deep Learning Models**

In this video, we‚Äôre going to look at some of the most important deep learning models. Along the way, I‚Äôll also introduce a few key terminologies you should be familiar with when studying deep learning or artificial intelligence.

Let‚Äôs begin with a quick summary. In the previous lecture, we discussed how artificial neural networks work. When you have an image, it is divided into multiple pixels, and each pixel becomes an input to a neuron. These neurons pass information to the next layer through connected pathways, and each connection carries a weight. Along with weights, we also use biases. The network has an input layer and an output layer, and if the output accuracy is correct, the model performs well. If the accuracy is not correct, we use backward propagation to adjust the weights and improve the performance. That‚Äôs the basic idea behind training a neural network.

Now, let‚Äôs explore the different types of deep learning models.
The first model we come across is the Convolutional Neural Network (CNN). As I always say, look at the root word: ‚Äúconvolution.‚Äù In simple English, convolution means something with twists and turns, and mathematically it refers to combining two signals to form a third signal. You can think of it as a deep learning technique that moves through various loops or transformations. CNNs are ideal for image and video processing. For example, if you want to classify whether an image contains a cat or a dog, you would use a CNN. Social media platforms also heavily rely on CNNs for image classification tasks.

The next model is the Recurrent Neural Network (RNN). Again, look at the root word: ‚Äúrecurrent,‚Äù meaning something that repeats. RNNs are designed for sequential or time-dependent data. For instance, if you want to predict tomorrow‚Äôs stock price, you may use inputs such as yesterday‚Äôs and today‚Äôs prices. The network uses weights and biases, applies an activation function, and produces an output‚Äîin this case, the predicted stock price for the next day. RNNs also work well with natural language tasks, such as predicting the next word in a sentence. That‚Äôs how auto-complete on your smartphone works; when you type ‚ÄúHow are,‚Äù it predicts ‚Äúyou?‚Äù automatically.

A special type of RNN is the Long Short-Term Memory Network (LSTM). LSTMs are excellent at handling long-range dependencies in sequences. A perfect example is Google Translate. When you type ‚ÄúHello, how are you?‚Äù in English, the system identifies the language and translates it to another language‚Äîsuch as French‚Äîwhile maintaining context. LSTMs enable this capability, and they also support voice-based translation.

Next, we have GANs (Generative Adversarial Networks), which have become incredibly popular in recent years. GANs consist of two neural networks: the generator and the discriminator. These networks compete with each other. The generator creates fake images or videos, while the discriminator tries to distinguish between real and fake. The goal is for the generator to produce results so realistic that even the discriminator cannot detect the difference. This technique is behind deepfake videos‚Äîhighly realistic but artificially generated footage of events that never actually happened. GANs make it possible to create photorealistic images and videos.

Finally, we come to one of the most important models in modern AI: the Transformer Model. This is the heart and soul of Generative AI (GenAI), which is extremely popular today. Transformers use a mechanism called self-attention to process sequential data more effectively than traditional RNNs or LSTMs. We will study transformers in detail in the next lecture, but for now, think of them as the foundation behind systems like OpenAI‚Äôs GPT-3. Models like ChatGPT use transformers to generate human-like text for applications such as chatbots, content creation, essay writing, poetry generation, and much more.

So this lecture was meant to build a basic understanding of the major deep learning models without diving too deep into the technical complexities. The key models to remember are:

CNNs for image and video processing

RNNs for sequential tasks like stock prediction or text auto-completion

LSTMs for long-range sequence learning, such as translation

GANs for generating realistic fake images and videos (deepfakes)

Transformers as the backbone of modern generative AI

With this, we come to the end of the lecture.

# **D) What is a Transformer Model?**

What if I tell you that what we are going to study today is the Transformer Model, which is the backbone of modern Generative AI? Everything you see today in the form of ChatGPT or any generative AI features offered by cloud vendors‚Äîat the heart and soul of all of it‚Äîis the transformer model. And this transformer model is a type of deep learning model that we should definitely be aware of.

So, what exactly is a transformer model?

As I always say, look at the root word. Whoever coined the term ‚Äútransformer model,‚Äù why did they call it transformer? Think back to your physics classes. In electrical circuits, a transformer is a device that transfers electric energy from one circuit to another. You have Circuit C1 and Circuit C2, and the transformer simply transfers the electrical energy from C1 to C2.

Now, when we talk about transformers in neural networks, the idea is similar. Transformers are a type of neural network architecture that have revolutionized the field of Natural Language Processing (NLP). This is truly a game changer. And once you see the architecture diagram of a transformer, you can easily correlate how information is transferred from one stage to another‚Äîjust like electrical energy is transferred between circuits.

So here is how the transformer architecture looks. You have an input layer, an output layer, a feed-forward network, and two important components: the Encoder and the Decoder. You may ask: what is an encoder? Encoders are neural network layers‚Äîusually multiple layers‚Äîthat process the input sequence and produce a continuous representation of it. You can consider this as the embedding of the input.

Then we have the Decoder. The decoder uses these embeddings to generate the output. Don‚Äôt worry if this feels abstract right now; we will understand it with examples. For now, simply remember that transformers use a unique architecture based primarily on self-attention.

Now, what is attention? For the moment, think of ‚Äúattention‚Äù as ‚Äúcontext.‚Äù This model understands context‚Äîit knows what you really mean. Transformers use self-attention to weigh the importance of different parts of the input. In other words, it can look at the entire sequence of data and decide which parts are more relevant when generating the output.

And here comes the biggest advantage of transformers:
Unlike RNNs or LSTMs, transformers process the entire sequence in parallel.
This is why ChatGPT responds so fast. You simply ask it to write a poem, an email, or a story, and within seconds it generates it‚Äîbecause everything is processed simultaneously. It has already been trained on humongous amounts of data, and from that training, it draws inferences instantly.

Compare this with RNNs. In RNNs, the model processes data step-by-step. For example, if you type: ‚ÄúHello, how are‚Ä¶‚Äù, the RNN looks at each word sequentially to predict ‚Äúyou?‚Äù. It‚Äôs more like the auto-correct or auto-complete feature on your phone. But transformers do not work like that‚Äîthey handle entire sequences together.

Now let‚Äôs come back to the important concept we hinted at: Attention.

The core of the transformer is the attention mechanism. Even if you don‚Äôt understand the full architecture, if you understand attention, you understand the essence of transformers. Attention allows the model to focus on the right parts of the input while producing the output. And this enhances the model‚Äôs ability to understand context and relationships.

Let‚Äôs look at an example. Take the word ‚Äúbank.‚Äù
In the sentence ‚Äúthe bank of the river‚Äù, ‚Äúbank‚Äù refers to the side of the river.
In the sentence ‚Äúmoney in the bank‚Äù, ‚Äúbank‚Äù refers to a financial institution.
The word is the same but the meaning is completely different.
This is where attention shines. The transformer remembers context‚Äîso it understands the meaning based on surrounding words.

Now let‚Äôs take another example: writing a story.
If you ask a transformer model, ‚ÄúWrite me a story,‚Äù 95% of the time stories begin with ‚ÄúOnce upon a time.‚Äù But how does the model decide that?

This happens through a concept called softmax, which converts scores into probabilities. Suppose the model has options like ‚Äúonce,‚Äù ‚Äúsomewhere,‚Äù ‚Äúthere,‚Äù or completely unrelated words like ‚Äúzygote.‚Äù It will give weightage to each option based on context. Since it knows you want a story, ‚Äúonce‚Äù becomes the most probable choice. Then it predicts the next word‚Äî‚Äúupon‚Äù‚Äîthen ‚Äúa,‚Äù then ‚Äútime,‚Äù and so on.

Again, this entire process happens in parallel, not step by step.

As we continue, remember two key components of transformers:

Encoder

Decoder

Each encoder and decoder consists of multiple layers called transformer blocks, and every block contains:

Self-attention layer

Feed-forward neural network

Also remember, transformers do not need recurrent layers, and this parallelism makes them extremely scalable. That‚Äôs why GPUs‚Äîwith thousands of cores running in parallel‚Äîare the preferred hardware for training transformer models.

Transformers have become the backbone of many state-of-the-art NLP models, such as:

Machine Translation (English ‚Üí German, etc.)

Text Generation

Summarization

Question Answering

Two popular transformer-based architectures are:

BERT ‚Äî Bidirectional Encoder Representations for understanding language

GPT ‚Äî Generative Pre-trained Transformer

And yes, the T in GPT stands for Transformer, which is exactly why we are studying this model now.

Beyond NLP, transformers are also making their way into computer vision and audio processing‚Äîanywhere tasks can be parallelized and context matters.

And with this, we come to the end of our lecture on the transformer model.

# **E) Demo: GANs - Deep Fake Video**

In the previous video, we talked about GANs, where we discussed the two main components: the generator and the discriminator. I also mentioned that one of the most popular applications of GANs is the creation of deepfake videos. Now, you might wonder‚Äîwhat exactly is a deepfake video?

A deepfake video is essentially a type of artificial intelligence technique used to create highly convincing fake images, audio clips, or videos. The term ‚Äúdeepfake‚Äù refers to both the technology behind it and the final output, which is completely fabricated. It‚Äôs not real content, which is why we call it ‚Äúfake,‚Äù and because it uses deep learning techniques, the word becomes ‚Äúdeepfake.‚Äù So, using ML and DL models, we are able to generate extremely realistic‚Äîbut entirely artificial‚Äîmedia.

Let‚Äôs look at an example to understand this better. There was an AI developer who created a deepfake video of Tom Cruise. I‚Äôm sure all of you know who Tom Cruise is. This video went viral across social media platforms like TikTok and Instagram, attracting millions‚Äîeven billions‚Äîof views. During a talk at TED, the host asked if they could have their own Tom Cruise deepfake video, and the developer actually shared one. In the clip, the AI-generated Tom Cruise says:

‚ÄúWhat‚Äôs up, Internet? I‚Äôm north of the border, at the TED conference. It‚Äôs not short for Theodore, but nobody calls me Thomas, so it‚Äôs cool. It‚Äôs Tom at TED. Yes, I Canada. Seriously though, everybody here‚Äîvery nice, very polite. Especially the waves.‚Äù

This short clip was a perfect demonstration of how realistic deepfake videos can be. Even though Tom Cruise never said those lines or attended that event, the AI-generated version looked and sounded almost identical to him.

Deepfake videos are typically created in one of two ways. The first method uses an original video of a target person, and the AI manipulates it to make them say or do things they never actually did. The second method involves swapping one person‚Äôs face onto another person‚Äôs body in a video. This process is commonly known as a face swap. Both techniques rely heavily on deep learning models to capture facial expressions, movements, and voice patterns, and then replicate them with stunning accuracy.

With this, I think you now have a clear understanding of what deepfake videos are, how they are created, and why GANs play such an important role in generating them.

# **F) Demo- Creating & Training Deep Learning Model**

Now it's time to walk through a quick demo based on the deep learning methodology we have learned so far. For this demonstration, we will use a dataset available on the Kaggle website. If you visit Kaggle, you will find the Pima Indians Diabetes Database. You may wonder: who were the Pima Indians? The Pima were a group of Native Americans living in the region that is now central and southern Arizona. Researchers collected medical data from this community and created a diabetes-focused dataset that is widely used in machine learning and deep learning experiments.

According to the description, this dataset originates from the National Institute of Diabetes and Digestive and Kidney Diseases. The main goal of the dataset is to predict‚Äîbased on diagnostic measurements‚Äîwhether a patient has diabetes. The dataset includes several health-related features for each patient and uses these measurements to determine whether the individual is diabetic or not. One important thing to note is that all the patients in this dataset are females and at least 21 years old, and all belong to Pima Indian heritage. This makes the dataset highly specific and controlled for certain demographic factors.

You can either download the data directly from Kaggle or use the copy found in the resources section. The dataset contains multiple columns, and the last column is extremely important: it contains the value 1 if the person has diabetes and 0 if they do not. As for the other columns, they represent various medical metrics. These include the number of pregnancies, plasma glucose concentration, blood pressure, triceps skinfold thickness, insulin levels, body mass index (BMI), diabetes pedigree function, and finally, the age of the patient. All of these factors together help determine whether a patient is likely to have diabetes.

With this understanding, we can now start building a deep learning model. We will create our own neural-network-based model and train it using this diabetes dataset. As always, it is best if you follow along. Start by opening Anaconda and launching a Jupyter Notebook. Once the notebook opens, create a new Python kernel and get ready to begin. Before we proceed, there are two very important terms you should know: Keras and TensorFlow. Keras is an open-source library that provides a Python interface for building artificial neural networks. It acts as a high-level API that makes deep learning easier to implement. TensorFlow, on the other hand, is a powerful open-source library developed by Google for machine learning and artificial intelligence, with a strong focus on training and deploying deep neural networks. Keras actually runs on top of TensorFlow, which is why we use the two together in this demo.

The first step is to import all the necessary Python libraries. Simply copy the import commands and paste them into your Jupyter Notebook. Once you run the cell, your environment will be ready. Next, we load the dataset. Here, you provide the path to your local file‚Äîthis path will differ depending on your system‚Äîand specify that the file is comma-delimited. After loading the data, we must split it into the input features (X) and the output variable (Y). The X values correspond to columns 0 through 7, representing all the predictor variables. The Y value is simply the final column, which indicates whether the individual has diabetes.

Once the data is ready, we define our Keras model. This involves creating a sequential model and specifying the layers of our neural network. After defining the model, we compile it. For compilation, we use the Adam optimizer and track accuracy as our performance metric. Compilation prepares the model for training. The next step is to train the model using the model.fit() command, where we pass in the X values, Y values, and specify the batch size. When you run this cell, the model begins learning from the data.

After training, we evaluate the accuracy of the model. When we run the evaluation step, we find that the model is approximately 77% accurate. If we train the model again, the accuracy may improve slightly‚Äîthis is a common behavior in deep learning. In this case, the accuracy increases from about 77.21% to 77.34%. This demonstrates why the size and quality of the dataset are so important. Larger datasets generally allow the model to learn more patterns and achieve better accuracy.

Next, we make predictions using the trained model. After generating the predictions, we compare the first 50 results to the actual expected outcomes. In many cases, the model correctly identifies whether the person is diabetic or not. However, there are also cases where the model fails. For example, the model may predict a 0 (non-diabetic) while the expected result is 1. These errors occur because the dataset is relatively small and the model has limited information to learn from. With a larger dataset, the accuracy would likely increase.

Through this demo, you can clearly see how deep learning works in practice. You have already studied the theory earlier, and now you can connect it with the hands-on implementation using Python, Keras, and TensorFlow. You‚Äôve seen how data is prepared, how the model is built, trained, evaluated, and then used for predictions. I hope this practical walkthrough has helped strengthen your understanding of deep learning.

# **IV) Generative AI Foundations (For Beginners)**

# **A) What is Generative AI ?**

So if you ask me today which is the hottest and most talked-about technology in the IT world, I would say without any doubt it is Generative AI. Now you might immediately ask, ‚ÄúBut what exactly is Generative AI?‚Äù‚Äîand that's a great question.

Before we dive into the meaning of Generative AI, I want you to recall the onion structure we discussed in our very first lesson. At the outermost layer, we had Artificial Intelligence. Then inside that, we had Machine Learning, which is a subset of AI. As we peeled further, we understood Deep Learning, which uses neural networks and forms a subset of Machine Learning. And now, inside that deep learning layer, sits Generative AI. So remember this important point:
Generative AI is a subset of Deep Learning.
Whatever concepts you learned in Deep Learning‚Äîespecially neural networks‚Äîthose form the backbone of Generative AI. This is why a strong understanding of Deep Learning always helps when you move into Generative AI.

So what is Generative AI? As always, I tell you to focus on the root word. The root word here is generate. Generate means to create, to produce, to build something new. So Generative AI is a type of AI that can actually create content‚Äîsomething new that did not exist before. And this content can be anything: text, images, music, videos, and even code. Yes, you can even ask a Generative AI system to write a Python program for you, whether it is a simple script or a complex piece of code.

Generative AI systems learn from extremely large datasets. So again, two things matter a lot here‚Äîquality and quantity. A large amount of good-quality data is needed because these models learn patterns, styles, structures and then generate new content based on what they have studied. This ability to understand complex patterns is what allows them to produce such high-quality outputs.

As mentioned earlier, the backbone of Generative AI is Deep Learning, especially neural networks. Most modern generative models, including the ones behind tools like ChatGPT, rely on deep neural networks trained on huge amounts of data. These networks work behind the scenes to generate responses, images, music, or whatever content you request.

Now let‚Äôs talk about the applications. Generative AI is used widely in art, where it can create stunning new images, paintings, or designs. In entertainment, it can create videos and animations. In marketing, companies use it to create ad content, taglines, and visuals. And in software development, developers can ask it to write entire code snippets or even debug existing code. So the possibilities are massive.

One of the biggest advantages of Generative AI is efficiency. Earlier, content creators had to write, rewrite, edit, and polish everything manually. Now, with generative systems, you simply provide a prompt or input, and within seconds, you receive an entire article, poem, presentation, or design. The creation process gets automated, reducing hours or even days of effort into minutes.

However, like any powerful technology, Generative AI comes with challenges, especially around copyright, originality, and misuse. Anyone can generate content and claim it as their own, which raises ethical concerns. So while using AI-generated material, we must be aware of these issues and use the technology responsibly.

Generative AI continues to evolve at a rapid pace. Every week, you will hear about new models being introduced‚Äîfaster, smarter, and more accurate than before. These systems start from a simple user prompt and expand it into a full piece of content. For example, if you say, ‚ÄúWrite a poem about my wife, whom I met in London,‚Äù and provide a few personal details, the AI will instantly produce a complete, heartfelt poem as if written by a poet. This is the power of user-prompted generation.

Finally, Generative AI is impacting multiple disciplines‚Äîjournalism, design, education, marketing, programming‚Äîand transforming how work is done. But again, understanding its impact and limitations is equally important.

So for now, the key takeaway is:
Generative AI = a subset of Deep Learning that focuses on creating new content.

In the next video, we will explore the difference between Predictive AI and Generative AI.

# **B) Predictive AI vs Generative AI**

Okay, so the million-dollar question that is always asked is:
What is the difference between Predictive AI and Generative AI?

Now what if I tell you that everything we have learned so far‚Äîwhether it was Artificial Intelligence, Machine Learning, or Deep Learning‚Äîmost of those topics were actually dealing with patterns. And based on these patterns, I told you repeatedly: What are you looking for?
Yes‚ÄîPredictions.

So all of that falls under Predictive AI.
But then what exactly is Generative AI?

As I told you in the previous lecture, the root word in Generative is generate, which means to create. So always remember this one golden sentence:
In Generative AI, you are creating new content.

Now let‚Äôs take a deeper look.

The function of Predictive AI is very clear‚Äîit is designed to analyze the existing data and make predictions. The root word here is predict. You are predicting future events or outcomes based on the data that you have trained the model on. And like I always say, when we talk about data, two things matter the most‚Äîquality and quantity. The better the data, the better the predictions.

Now let us talk about Generative AI.
Generative AI focuses on creating new content. It creates new data that resembles the training data, and sometimes even produces completely original output. This could be text, this could be images, videos, music, or even code. So yes, you can ask a Generative AI system to write a Python program for you as well.

When we look at usage, Predictive AI is mainly used for data analysis. It often involves statistical methods and machine learning algorithms to identify patterns and trends in historical data. For example, when you studied image classification‚Äîdistinguishing between a cat and a dog‚Äîor when we talked about the Iris flower dataset where we predicted whether a flower is Iris Setosa, Iris Virginica, or Iris Versicolor based on features. All of that is Predictive AI.

We also discussed stock market prediction, where models try to predict the future price of a stock. Again‚Äîbased completely on historical data patterns.

But things change when we enter the world of Generative AI.
Here, the focus is on content creation. Generative AI can generate text, images, music, synthetic data, simulations‚Äîanything that involves creating new output. You give a prompt, and it creates something for you.

Let‚Äôs talk about applications.
Predictive AI is used in areas like weather forecasting, stock market prediction, market price forecasting, risk assessment, customer behavior prediction, and fraud detection. These are all cases where we want to know ‚ÄúWhat will happen next?‚Äù

Generative AI, on the other hand, is used in art, design, media creation, synthetic dataset generation, and creating realistic simulations. Anywhere new content is being created, that is where Generative AI shines.

Predictive AI also helps in providing insights, recommendations, and better decision-making. For example, we saw recommender systems‚Äîlike Netflix suggesting movies or Udemy suggesting related courses. These systems analyze your data and predict what you might like next. This is Predictive AI.

But Generative AI has the ability to produce creative and novel outputs that go beyond the input data. If you want a poem, if you want a letter, if you want an essay‚Äîjust provide a few words, and Generative AI expands them into beautiful, full-length content. That is the magic of generative models.

Now a very important conceptual point:
Predictive AI is reactive in nature, while
Generative AI is proactive in nature.

Predictive AI reacts based on existing data and tells you what is likely to happen next. It is only analyzing.
Generative AI proactively creates new data points, new content, new ideas. You are not just analyzing anymore‚Äîyou are generating.

For more examples, Predictive AI includes credit scoring models, demand forecasting, supply chain prediction, and predictive maintenance.
Generative AI includes ChatGPT and GPT models for text, DALL¬∑E for images, AI-based music composition, synthetic data generation, and AI code generation.

With this lecture, I believe you now have a very strong and clear understanding of the differences between Predictive AI and Generative AI.

# **C) What is LLM ?**

Okay, so one of the most important concepts that we study in Generative AI is something called LM. Now everywhere you go, you will hear the word ‚ÄúLM.‚Äù But what exactly is LM?

LM stands for Large Language Models.
As the name suggests, these models are really large. They are not small machine learning models that you may have seen earlier. Here we are talking about models with multi-millions or even billions of parameters. That is why they are called large and why they are so powerful.

And since the word ‚Äúlanguage‚Äù appears in the name, that tells you they are related to NLP‚ÄîNatural Language Processing, which we studied earlier. So what are LLMs?
LLMs are sophisticated AI systems designed to process, understand, and generate human language.

This means that whenever you give an input‚Äîfor example, ‚ÄúWrite a poem‚Äù or ‚ÄúCreate a short story‚Äù‚Äîthe LM is able to understand your request, process it, and then generate human-like text based on that. That is why LLMs are at the heart of modern Generative AI.

Now let‚Äôs talk about the most important aspect: scale and complexity.
The word large is not just for show. As the number of parameters increases, the complexity of the model also increases. When I say parameter, don‚Äôt worry‚Äîwe will go deeper into what parameters actually mean. For now, just understand that LLMs are composed of millions or billions of these parameters, which is what makes them capable of producing such intelligent and human-like responses.

The best example is GPT-4 by OpenAI, which has an extremely large number of parameters and can generate very realistic and coherent text.

But now the question comes: What are parameters?
Let me give you a simple analogy. Imagine you are building a house. The house will have different parameters‚Äîlike the number of rooms, the size of each room, the layout, the design, the type of flooring, the color of the walls. All of these are parameters of your house.

Now translate this to a large language model. Its parameters relate to what it can do. For example, the model may have the ability to generate text, translate languages, summarize articles, write poems, answer questions, create stories, or write letters. These are only small examples‚ÄîLLMs have billions of such internal parameters that allow them to perform a huge range of tasks. This is why they become so complex.

LLMs are trained on massive datasets‚Äîfar beyond normal sample data you might use in machine learning. They are trained on a wide range of internet text. For example, GPT-4 is trained on huge collections of books, articles, websites, and other diverse text sources. This vast training data helps the model understand grammar, context, reasoning, and even creativity.

Now let‚Äôs look at some popular LLMs. The list keeps expanding, but some of the major ones are:

‚Äì From OpenAI: GPT-3, GPT-4

‚Äì From Google: Bard (earlier) and now Gemini, which is becoming very popular and may soon be available on mobile devices

‚Äì Cohere, which is widely used in Oracle Cloud (OCI)

‚Äì LLaMA from Meta (Facebook), which is open-source and heavily used in research

These are some of the leading large language models in the industry today.

Now why do we use LLMs?
Because they help in natural language understanding and natural language generation. That means they understand human language and can respond in the same style. They can write essays, poems, simulate conversations, answer questions, and create human-like text in many forms.

The applications are huge. LLMs can be used to build chatbots, create content, translate languages, summarize documents, analyze text sentiment, and much more. For example, GPT-4 currently powers some of the most advanced chatbots used around the world.

Because they are built using machine learning‚Äîmore specifically deep learning‚Äîthese models are constantly improving. Remember, with deep learning we studied neural networks, which mimic how the human brain works. LLMs use a special type of neural network architecture called transformers, which we already studied.

This architecture allows them to process sequences of data efficiently. GPT-4, for example, is based on transformer architecture, which makes it great at understanding long paragraphs, context, and relationships between words.

A very important point is that LLMs are designed for continuous learning and improvement. They keep improving over time as they get more feedback, more usage, and more diverse prompts from users all around the world. That is why their performance gets better over time.

Another advantage is customization. You can use an existing pre-trained model, or you can customize (fine-tune) it for industry-specific tasks. For example, GPT-4 can be fine-tuned for legal language, medical reports, financial analysis, and many specialized domains. This is very useful when you want highly accurate output for a specific field.

Now if we look at this simple architecture diagram, we can see different types of inputs such as text, structured data, or even voice. These inputs go to the training model. The large language model adapts using the training data, learns continuously, and generates outputs. These outputs can include instructions, information, object recognition, image captioning, Q&A, quizzes, or even sentiment analysis.

For example, when we study OCI language services, we will see how LLMs perform tasks like identifying whether a text expresses positive, negative, or neutral sentiment.

So by now, I believe you have a very clear understanding of what an LM‚Äîor Large Language Model‚Äîis, how it works, what it can do, and why it is such an important part of Generative AI.

# **D) What is Embedding ?**

Embeddings are a really important concept in generative AI and are considered the backbone of it, especially when you study systems involving vectors, vector databases, or concepts like RAG. Embedding is essentially a way of converting the features of an object, such as a word, into a vector of real numbers, and while this is a theoretical definition, the main idea is that you convert the features of an object into numbers because vectors are nothing but numbers. To better understand this, consider a diagram using the example of a cat. A cat has several features, and we can assign numbers to each feature, such as whether it is a living being, belongs to a particular family like tigers or lions, whether it is human, its gender, whether it represents royalty, whether it is a verb, or whether it is plural. For each of these features, an embedding algorithm assigns specific numeric values ‚Äî for example, the feature ‚Äúliving being‚Äù may get 0.6, while something like ‚Äúroyalty‚Äù may get a negative number that indicates it's not royalty. 

Similarly, words like houses get a positive value under plural, and the word kitten gets values very close to cat because both share similar features. These features form a seven-dimensional representation since there are seven features, but working in high dimensions is difficult, so we apply dimensionality reduction, where these embeddings are reduced from 7D to 2D. Dimensionality reduction transforms complex high-dimensional data into a simpler lower-dimensional space for easier visualization while still preserving as much significant information as possible. After reduction, we can visualize the embeddings in a 2D space; words with similar meanings or related relationships will appear close together‚Äîlike cat and kitten‚Äîwhile unrelated words like dog or houses appear farther away. You can also observe relationships such as man-woman or king-queen, where the algorithm captures similarities in features like being a living being, being human, or having a certain gender. This demonstrates how generative AI systems understand context, meaning the system recognizes both meaning and relationships between words. The process is: start with a word, convert it into a word embedding (a numerical vector), reduce it from a high-dimensional representation to 2D, and visualize it in 2D where similar or related items cluster together. Embeddings transform complex data such as words or images into numerical vectors that computers can understand because computers work with numbers. 

These vectors help the system understand similarities‚Äîfor example, cat and kitten are closer than cat and house. They also simplify complex data by breaking many features into smaller, easier-to-process numbers. Embeddings are used everywhere, such as in language understanding and recommender systems, and popular embedding methods include Word2Vec by Google, GloVe (Global Vectors), and BERT (Bidirectional Encoder Representations from Transformers). Computers learn to create embeddings by training on large amounts of data, and better data leads to better embeddings. 

Advanced embeddings can also understand context, such as distinguishing between the word ‚Äúbank‚Äù used as a riverbank and ‚Äúbank‚Äù used for money. For example, if you were to place an apple in a 2D embedding space, it would fall near other fruits like strawberry and banana based on shared features, just like sports items like football and basketball cluster together, or vehicles with wheels like bicycles and cars cluster in their own category. Overall, embedding is simply a way of converting the features of an object‚Äîlike a word‚Äîinto a vector of real numbers so that machines can understand meaning, similarity, and context. In the next step, you can learn about vectors in more detail.

# **E) What is a Vector Database ?**

Okay, so in the previous video we understood what embeddings are. Now it‚Äôs time to take a look at vectors and vector databases. A vector database is a type of database designed specifically for storing, indexing, and querying vectors, which are arrays of numbers representing data in a high-dimensional space. If we recall what we learned about embeddings, when we take a word like ‚Äúcat,‚Äù we extract its features and assign numbers to each feature. These sets of numbers form what we call vectors. Essentially, all we are doing next is storing these vectors‚Äîthese arrays of numbers or embeddings‚Äîinto a database, and that database is known as a vector database.

You can imagine this by comparing it to relational databases like Oracle or MS SQL Server, where you store tables of relational data. In contrast, vector databases store vectors or embeddings. When you look at the diagram, you can see that different kinds of inputs can be processed. The input could be a document, a picture, a sound file, or even a video. These inputs are passed through different transformers: an NLP transformer for text, an audio transformer for sound, and so on. The transformer generates a set of numbers‚Äîembeddings‚Äîand these vectors are then stored inside the vector database.

Once data is stored, you can query it, index it, and perform searches, just like you would with any other database. The purpose is straightforward: vector databases are primarily used for storing embeddings that represent complex data like images, text, or audio in a numerical form that machines can understand and process. This is exactly what we discussed earlier‚Äîembeddings exist to break down complex objects into smaller numerical representations. For example, when we studied the example of the cat with seven dimensions being converted into a two-dimensional space, we saw how high-dimensional information can be reduced while keeping significant meaning.

These vectors are stored so we can perform similarity searches. Examples include ‚ÄúIs a cat similar to a kitten?‚Äù or ‚ÄúHow close are they positioned in vector space?‚Äù Similarly, ‚ÄúIs a man related to a woman?‚Äù or ‚ÄúIs a king related to a queen?‚Äù These kinds of comparisons are done through similarity search mechanisms that vector databases are optimized to perform. They allow quick retrieval of items most similar to a given query. Some common similarity measures include Euclidean distance and cosine similarity, both of which look at how close vectors are in their multi-dimensional space.

Vector databases are widely used in machine learning, artificial intelligence, and recommendation systems. Think about Netflix‚Äîrecommendation systems depend heavily on similarity search, such as finding what kind of movies you like based on prior viewing. Vector databases help handle very large volumes of data because the data, once converted into embeddings, becomes compact, structured, and easier for machines to work with. They support scalability, efficient querying, and are designed for big data applications.

Another important thing is that vector databases can be integrated with machine learning models. This allows them to directly store and query embeddings generated by these models. Most modern vector databases are built to work seamlessly with ML pipelines. They provide high-performance, real-time search and retrieval, which is essential for interactive applications and services. Just like traditional databases let you query tables, vector databases let you query vectors, but the purpose here is specifically to perform similarity searches.

So with this, we come to the end of understanding vector databases. In the next video, we will explore the relationship between embeddings, vectors, and vector databases. Thanks for watching.

# **F) Embedding vs Vector Database**

A lot of times when we introduce concepts like embeddings, vectors, and vector databases, people get confused because they aren‚Äôt able to clearly understand the differences between an embedding, a vector, and a vector database. That is why it becomes important to break them down and see how they are actually connected to each other. The best way to understand this is to always begin with the real world data. Real world data can be in the form of text, images, videos, or sound. All of these represent raw inputs that we want a machine to understand.

As we studied earlier, embedding is the process that transforms real world data into numerical numbers or vectors. These embeddings capture the important features of the data. For example, in the cat‚Äìkitten example, we took real world data and used embeddings to create numerical vectors. Embedding is the step that converts raw, complex information into usable numerical representations that a machine can process.

These vectors represent the features of the data. Vectors are nothing but the numerical representation of data created through the embedding process. Each vector is a list or array of numbers that encodes the characteristics of the data. When we saw the example of the cat‚Äîits category, whether it is feline, male or female, etc.‚Äîeach of those features became part of the vector. From a high-dimensional representation like seven dimensions, we were able to reduce it to two dimensions, but the core idea remains the same: vectors are the numerical feature-based representation of the object.

Once you have these vectors, you need to store them somewhere. Just like you store your regular relational tables in Oracle or SQL Server, you store vectors in a vector database. A vector database is a special kind of database designed to store and efficiently retrieve large collections of vectors. It uses optimized techniques to quickly find vectors that are similar to each other. This is what enables similarity searches‚Äîlike identifying that a man is related to a woman, a king to a queen, or a cat to a kitten.

We also discussed how vectors help group similar items, like fruits. If you create a vector for an apple, it will naturally fall close to other fruits because of shared characteristics. The vector database simply stores these vectors and retrieves them efficiently when needed. The flow is simple: real world data is transformed through embeddings to create numerical vectors, and these vectors (which represent the features of the data) are then stored in a vector database.

Finally, all of this happens in the backend, but the user interacts with it through an application at the frontend. Vector databases are used in various applications, especially similarity search, where we look for data points that resemble one another. A common example is Netflix, where the recommender system finds movies or shows similar to what you like. The same principle applies to many other recommendation systems. With this, you now have a clear understanding of the relationship between embeddings, vectors, and vector databases.

# **G) What is Retriever Augmented Generation (RAG) ?**

Hello folks, today we are going to study a very important concept within AI and generative AI called RAG, which stands for Retriever-Augmented Generation. At first, the term might sound a bit complex, but as I always suggest, it helps to break the word down and understand the root meaning. Whoever coined the term clearly had a logical idea behind it. The word retriever suggests fetching something, generation refers to creating something, and augment means adding value. So, RAG essentially means retrieving information, augmenting it with additional intelligence, and then generating a meaningful response.

To understand this, consider a simple example from the non-AI world. Suppose you are doing a PhD and you need to write a research paper. What would you do? You would go to a library, browse the internet, and gather information from various books and sources. These sources act as your knowledge base. Once you collect the information, you combine it all and write your paper. RAG works in a similar way. A user asks a question, the system looks into its knowledge base‚Äîwhich in our context is often a vector database‚Äîand retrieves the most relevant information. This is the retrieval stage.

Once the relevant information is retrieved, that retrieved text is combined with the user‚Äôs query. Together, they form a complete prompt or input. This prompt is then passed to a large language model such as ChatGPT or Gemini. The LLM uses both the user‚Äôs question and the retrieved content to generate a high-quality response. So in RAG, you are using the best of both worlds: your organization‚Äôs internal data as the source of truth and the power of the LLM to produce a refined and contextually accurate output. That is the essence of Retriever-Augmented Generation.

A natural question is: where do we actually use RAG? Think of a bank or any enterprise company. They have private, sensitive data stored inside their internal infrastructure. They do not want external LLMs like ChatGPT to directly access their confidential information. This is where vector databases step in. The retrieval happens from internal, secure data sources, while only the generation is outsourced to the LLM. In this way, RAG allows organizations to maintain full control over their data while still benefiting from generative AI capabilities.

In simple theory, RAG uses a retriever to find the relevant information and a generator to create the response. For example, if you ask, ‚ÄúWhat is climate change?‚Äù, RAG retrieves relevant scientific articles stored in your internal knowledge base and then uses the LLM to create a clear, concise explanation. This significantly improves response quality. Earlier, developers often had to hard-code responses or manually prepare structured rules. But with RAG, the system dynamically searches your source data in real time and generates accurate, detailed answers.

Another example is a customer support chatbot that uses RAG to fetch product details from an internal database. This ensures the responses are precise and fully aligned with company-specific information. RAG is highly adaptable across domains‚Äîcustomer service, education, enterprise chatbots, and any application where factual accuracy and context are essential. Organizations often cannot rely solely on the LLM‚Äôs built-in knowledge, so RAG ensures responses remain grounded in their own internal documents.

RAG also enhances contextual understanding. Throughout our discussions, we‚Äôve emphasized the importance of context. With RAG, the context is retrieved directly from enterprise data sources, and the LLM interprets that context correctly. This helps avoid mistakes that purely generative models sometimes make, such as mixing up meanings‚Äîlike ‚Äúbank‚Äù as a financial institution vs. ‚Äúbank‚Äù of a river. By grounding the generation in real documents, RAG ensures responses are accurate and contextually appropriate.

Let‚Äôs understand how RAG works in practice. First, you have your data sources: documents, PDFs, images, videos, and so on. All these are passed through an embedding model, which converts them into numerical vectors. These vectors are then stored in a vector database. This covers steps one and two: embedding the data and storing it in the vector DB. Next comes the user query. When the user asks a question in the chatbot, the query is also passed through the same embedder to generate a vector. This allows the system to perform a semantic search, not a simple SQL-style search. Instead of matching exact keywords, semantic search finds documents that are conceptually similar.

Once the relevant documents are found, they are sent to the LLM along with the user‚Äôs question. The LLM uses both pieces of information‚Äîthe retrieved documents and the original query‚Äîto generate a final response. This combined context becomes the prompt for the LLM. The response is then returned to the user. So the workflow is: user submits a query ‚Üí query is converted into a vector ‚Üí similar documents are retrieved ‚Üí both query and documents are processed by the LLM ‚Üí final answer is generated.

This is the complete process behind Retriever-Augmented Generation. With this understanding, you can confidently explain RAG to anyone and describe exactly why it is so important for organizations today.

# **H) What is Langchian ?**

Today, we are going to understand a key concept in the world of generative AI called LangChain. To make it easier to grasp, think of LangChain like Lego. Most of us or our children have played with Lego. Lego consists of small bricks and comes with predefined instructions to build specific models. You can follow these instructions to build the model exactly as shown, or you can use the same bricks to create something entirely original based on your imagination. Similarly, LangChain allows you to use pre-built modules or models for common tasks or combine them in new ways to create complex language applications tailored to specific needs.

Just as Lego bricks are assembled following structured guides to create a final model, LangChain integrates various data sources with AI language models. For example, you might want to combine multiple LLMs such as LLaMA and ChatGPT. LangChain makes this integration possible. Essentially, LangChain is a technology framework that enables developers to build applications with large language models efficiently. You don‚Äôt need to start from scratch; instead, you can use pre-built modules, customize them, and assemble them as per your requirements.

A practical example of LangChain in action is the implementation of RAG (Retriever-Augmented Generation). Suppose you have an external data source, which is converted into embeddings and stored in a vector database. When a user query comes in, the same vector database is used to retrieve relevant information. This retrieved data acts as a prompt that is then sent to the language model. So, by using LangChain, you can easily implement RAG workflows. LangChain is modular and flexible, allowing developers to mix and match modules, which simplifies incorporating sophisticated AI functionality into applications.

LangChain is also very developer-friendly. Without it, integrating LLMs into software applications could become highly complex. The framework reduces implementation difficulty and enables developers to focus on building functionality rather than managing the underlying complexity of AI-driven conversational agents.

Here‚Äôs a simple example to make it concrete: imagine a user asks, ‚ÄúWhat is the weather like in Chandigarh today?‚Äù through an app integrated with LangChain. ChatGPT alone cannot provide current weather information since its knowledge is limited to its training data (e.g., GPT-4 is trained up to December 2023). LangChain processes the query, understands that it requires current weather data, and retrieves the information from the appropriate external source or API. Suppose the weather database returns sunny, 40¬∞C.

Next, the language model comes into play. LangChain sends the retrieved data as a prompt to the LLM, which generates a natural, user-friendly response, such as: ‚ÄúThe weather in Chandigarh today is sunny with a temperature of 40¬∞C.‚Äù This response is then delivered to the user through the app. In this way, LangChain orchestrates the workflow: the query is processed, data is retrieved from the appropriate source, and the language model generates the response in a clear format.

In summary, LangChain is a technology framework designed to integrate large language models into applications effectively. It allows developers to use pre-built modules, customize workflows, combine multiple LLMs, and retrieve and generate responses using external or private data sources. LangChain makes it easier to implement complex AI-driven applications, such as RAG-based systems, while remaining flexible, modular, and developer-friendly.

# **I) Role of Langchain in RAG**

By now, we have understood the concept of RAG (Retriever-Augmented Generation) and have also touched upon LangChain. At this point, it‚Äôs common to feel a bit confused between RAG and LangChain. To clarify, LangChain is essentially the heart and soul for achieving a real RAG system. LangChain is tightly integrated into both the retrieval and generation components of RAG, making the end-to-end workflow possible.

The workflow begins with document retrieval and ingestion. First, you ingest your data into the system. This could be enterprise knowledge bases, sets of PDFs, or other document formats. LangChain facilitates this entire process, acting as the framework that enables retrieval from the enterprise knowledge base. Without LangChain, orchestrating this process would be highly complex.

Once the documents are retrieved, the next step is processing and embeddings. Each sentence, paragraph, or word in the documents is converted into vector embeddings, which are arrays of numbers representing the features of the text. LangChain plays a key role in ensuring that this embedding process is integrated seamlessly into the workflow.

After creating embeddings, these vectors need to be stored for efficient querying. This is where a vector database comes in. LangChain helps manage the storage of document embeddings in the vector database, which is then used to perform similarity searches. When a user submits a query, the query itself is converted into embeddings, which are then compared with the document embeddings to find the most relevant information.

Once the similarity search identifies the relevant documents, the next step is query enhancement and context preparation. LangChain helps combine the original user query with the retrieved information to create an enriched context. This enriched context becomes the prompt that is sent to the language model (LM). By doing this, the LM has all the necessary context to generate accurate and informed responses.

Finally, in the generation phase, the LM creates the response based on the prepared prompt. LangChain manages the interaction with the LM, ensuring that the input is correctly formatted and that the generation process is executed effectively. Essentially, LangChain provides an end-to-end framework that integrates every component of the RAG system‚Äîfrom the initial user query, through retrieval, embedding, similarity search, and context preparation, to the final response generation by the LM.

With this explanation, it becomes clear that LangChain is critical to a RAG system, serving as the backbone that enables seamless integration and smooth operation across all stages of the workflow.

# **J) Prompt Engineering & Fine Tuning**

After getting a good understanding of what a Large Language Model (LM) is, it‚Äôs time to understand two more important concepts: prompt engineering and fine tuning. These are crucial from the perspective of generative AI or LMs, so having a clear understanding of them is very important.

Let‚Äôs start with prompt engineering. First of all, remember that a prompt is essentially an input. Prompt engineering is the practice of crafting or creating inputs (prompts) for a generative AI model to elicit the best possible output. Prompts are the inputs to a model, and in generative AI, the principle is simple: if your input is poor, your output will be poor; if your input is good, your output will be good. With prompt engineering, you are essentially experimenting with different ways to ask or phrase a question.

For example, you could ask, ‚ÄúWhat is an antibiotic?‚Äù or ‚ÄúGive me some details about antibiotics,‚Äù or ‚ÄúCan you summarize antibiotics?‚Äù Another example could be giving a statement like, ‚ÄúHere are some lines about antibiotics, is my understanding correct?‚Äù These are different ways of prompting the model, and by experimenting with these different prompts and observing the outputs, the model‚Äôs performance and understanding can improve over time.

Prompt engineering is a practice that involves using specific keywords, structured sentences, and including examples to guide the AI. Ultimately, the goal is to maximize the AI‚Äôs understanding of the task and generate more accurate, relevant, and creative responses. The prompt should be designed in a way that the AI can understand and process it effectively to produce meaningful human-like text. This also requires understanding context; for example, the word ‚Äúbank‚Äù could refer to a riverbank or a financial bank, and the AI must interpret it correctly based on context.

Prompt engineering can be used by both novices and experts. A novice may use trial and error: they provide a prompt, observe the output, refine the prompt, and try again until they get satisfactory results. Experts, on the other hand, know how to craft prompts in a highly precise way to get accurate and creative outputs, often using sophisticated methods informed by a deep understanding of the model.

The second concept is fine tuning. In simple terms, fine tuning is the process of optimizing the performance of a pre-trained AI model to better suit specific tasks or datasets. This involves training the AI on a particular dataset to refine its responses so that they are more aligned with the characteristics of that data. In this process, both the quality and quantity of data are extremely important. Pre-trained models like large language models are trained on massive datasets from books, websites, Wikipedia, and more, which gives them broad knowledge.

Fine tuning requires more computational resources and technical knowledge than prompt engineering because it involves training with large datasets and making adjustments to the model‚Äôs parameters. It enables the creation of custom AI models that can perform better on specialized tasks. Fine tuning is not a one-time activity; it‚Äôs a continuous process where models are periodically updated as they are exposed to new data.

One challenge with fine tuning is overfitting. Overfitting occurs when a model performs extremely well on the training data but fails to generalize to new, unseen data. Therefore, careful monitoring and adjustments are needed to ensure that the model performs well not just on the trained dataset but also on real-world inputs.

Prompt engineering and fine tuning often go hand in hand. Fine tuning provides a strong baseline performance for the model, optimizing it for the overall task, while prompt engineering optimizes individual interactions, guiding the model to produce the most accurate and creative outputs for specific prompts. Both techniques are crucial for enhancing the performance of generative AI models in various applications and are used together to ensure the model is both well-trained and adaptable to diverse user inputs.

In summary, prompt engineering is about crafting effective inputs to get the best output, while fine tuning is about optimizing the model itself for specific tasks. Both require attention to context, data quality, and experimentation, and when used together, they significantly enhance the capabilities and performance of generative AI models.

# **V) AI Infrastructure**

# **A) What is a GPU ?**

In this video, we will discuss the $1 million question. Why the $1 million question? Just look at how the world is going crazy about AI. The backbone of AI, in terms of infrastructure, is the GPU. Just look at Nvidia shares‚Äîthey are skyrocketing. This has everyone curious to understand what a GPU is. Most people know about a CPU, which has been used for ages, but why have GPUs suddenly become so important in our lives?

A GPU stands for Graphics Processing Unit. You may wonder why we are talking about a graphics unit in the context of AI. GPUs were originally designed to accelerate graphics rendering, mainly for gaming, which is why some people even call it a Gaming Processing Unit. They were designed to render graphics in applications such as 3D games and animations. The key feature of a GPU is its highly parallel structure. The biggest difference between a CPU and GPU is that CPUs perform computations serially, while GPUs perform computations in parallel. GPUs have a parallel processing architecture, allowing them to process many computations simultaneously. Tasks that take longer on a CPU due to serial processing can be completed much faster on a GPU.

Rendering, also known as image synthesis, is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model using a computer program. Originally, GPUs were used for this purpose in video games and 3D animation, long before AI became a consideration. However, scientists later realized that the parallel processing power of GPUs could be leveraged in machine learning and AI. Neural networks, for instance, involve multiple interconnected nodes, and GPUs, with their thousands of cores, can handle multiple operations concurrently. This parallelism significantly accelerates the training of complex neural networks.

Moreover, GPUs are more energy-efficient than CPUs for parallel processing tasks. While CPUs can also run multiple cores, their architecture is not as efficient as GPUs for breaking down tasks into multiple subtasks to be executed simultaneously. This results in a better performance-per-watt ratio with GPUs.

There are two main types of GPUs: integrated and dedicated. Integrated GPUs are built on the same chip as the CPU, sharing the same memory and resources. Dedicated GPUs, on the other hand, are separate cards with their own memory and processing power, offering superior performance. GPUs are essential for powering virtual reality and augmented reality applications, where speed and performance are critical for an immersive experience. Wherever there is a large amount of data or graphics to process, GPUs are indispensable.

The technology behind GPUs is rapidly evolving, with constant innovations leading to faster, more efficient, and more powerful GPUs. Major companies in the GPU space include Nvidia, AMD, Intel, and ARM. Currently, Nvidia is the market leader, with AMD catching up. There is significant investment in the GPU industry due to its critical role in AI and graphics-intensive applications.

To summarize, CPUs (Central Processing Units) typically have a few cores‚Äîranging from 8 to 100‚Äîand are optimized for low-latency, serial processing tasks. GPUs, in contrast, have hundreds or thousands of cores, optimized for high-throughput, parallel processing. CPUs perform well for tasks requiring sequential computation, while GPUs excel at parallel processing tasks such as graphics rendering, 3D games, machine learning, and deep learning. A CPU processes tasks one at a time, whereas a GPU breaks tasks into smaller subtasks and processes them simultaneously, making it ideal for operations that require high concurrency.

Traditional programming was mostly written for CPUs, so additional software and programming are needed to utilize GPUs efficiently for parallel execution. This is an important consideration for developers who want to leverage GPU power. With this, you now have a clear understanding of what a GPU is. You can also look up images of Nvidia processors to see how GPUs look in the real world. In the next video, a demo will clearly illustrate the difference between CPU and GPU.

# **B) Demo: CPU Vs GPU**

Hello and welcome. I‚Äôd like to share a video that was an eye-opener for me. When I was investigating the difference between a CPU and a GPU, I realized that while most of us are familiar with what a CPU is, many people might not be aware of what a GPU does. In the previous lecture, we discussed the theoretical differences between a CPU and a GPU, but now I want to show you a demo that illustrates it visually.

The video I‚Äôm sharing is from Nvidia and was uploaded almost ten years ago when they introduced GPUs to the world. In this demo, the presenter paints a picture of how a CPU operates as a series of discrete actions performed sequentially, one after the other. Then, he compares it to how a GPU works with thousands of cores running in parallel.

In the demonstration, when the trigger is hit, 2,100 gallons of air go through accumulators, out through valves, and into 1,100 tubes. Each tube contains a paintball, which flies across seven feet of space and reaches its target in just 80 milliseconds. The aim is to paint the Mona Lisa. While a CPU would handle tasks sequentially, this setup represents a GPU‚Äôs parallel processing, where thousands of operations occur simultaneously to achieve a fast, coordinated result.

This demo is a clear and engaging way to visualize the power of GPUs compared to CPUs. It highlights how GPUs, with thousands of cores running in parallel, can perform complex tasks much faster than CPUs, which handle tasks sequentially. Thanks for watching, and I hope you enjoyed the demonstration as much as I did.

# **C) What is RDMA Cluster Network**

After having a good look at High-Performance Computing (HPC), it is time to explore RDMA cluster networks. You might recall that HPC involves using multiple computers in a cluster to perform tasks faster than a single computer could. By adding thousands of computers to a cluster, you can complete more work in less time. In HPC, these computers are interconnected, often with the fastest CPUs and GPUs, but a slow network can become a major bottleneck. To address this, we have RDMA cluster networks.

RDMA stands for Remote Direct Memory Access. Breaking it down: ‚ÄúRemote‚Äù means accessing memory on a different computer rather than locally, and ‚ÄúDirect Memory Access‚Äù means the data transfer happens directly between memories without involving the CPU or operating system. RDMA cluster networks are specialized network configurations designed specifically for HPC to ensure high-speed communication between cluster nodes.

To understand the benefit of RDMA, consider how two computers normally communicate. Data from one application on a computer passes through several layers‚Äîsuch as the OS, kernel, and network interface card (NIC)‚Äîtravels over the network, and then passes up the layers of the receiving computer to reach memory. This process introduces latency and slows down performance. RDMA bypasses these layers, allowing direct memory-to-memory data transfer between computers. This reduces CPU overhead, lowers latency, and increases throughput, which is crucial for tasks requiring rapid data processing, such as AI and big data analytics.

RDMA networks are known for their extreme low latency and high throughput, making them ideal for HPC applications. They can be implemented over various network protocols, including InfiniBand, which has been used in Oracle Exadata systems for years. More recently, RDMA over Converged Ethernet (RoCE or Rocky) enables RDMA over Ethernet, and iWARP provides RDMA over wide area networks. These implementations make RDMA highly versatile and adaptable to different environments.

Another significant advantage of RDMA networks is scalability. HPC clusters often grow over time, and RDMA supports seamless expansion, making it suitable for research and enterprise settings where computing demands can increase. RDMA is also beneficial in clustered databases and storage systems, enabling rapid and efficient data transfer between nodes, which is essential for cloud computing, parallel computing, scientific simulations, real-time analytics, and other HPC applications.

RDMA is also energy-efficient. By bypassing the CPU and reducing processing overhead, it optimizes data transfer and consumes less energy than traditional networks. However, there are cost considerations. RDMA cluster networks require specialized hardware and maintenance, making them more expensive than traditional networking solutions. While the performance benefits are significant, implementing RDMA requires investment in infrastructure.

In summary, high-performance computing requires a fast network to prevent bottlenecks, and RDMA cluster networks provide the solution. RDMA allows one computer‚Äôs memory to communicate directly with another‚Äôs, bypassing OS layers, CPUs, and NICs. This direct memory access improves speed, efficiency, and scalability, making RDMA essential for modern HPC and cloud computing applications.

