# Generative-AI-Beginner-to-Pro-with-OpenAI-Azure-OpenAI-Notes

**I) AI Concepts and Workloads - (For Absolute Beginners- Optional)**

**A) What is AI ?**

**B) History of AI**

**C) Benefits of Artificial Intelligence (AI)**

**D) Types of AI Workloads**

**E) AI vs ML vs DL**

**II) Machine Learning Foundations (For Absolute Beginners-Optional)**

**A) Machine Learning - Real World Example**

**B) Machine Learning - Key Terminologies**

**C) What is Machine Learning ?**

**D) Types of Machine Learning**

**E) What is Supervised Machine Learning ?**

**F) What is Classification in SML ?**

**G) What is Regression in SML ?**

**H) What is Unsupervised Machine Learning ?**

**I) What is Re-inforcement Machine Learning**

**J) What is a Jupyter Notebook**

**K) Demo: Install Anaconda**

**L) Demo: Understanding the IRIS Dataset**

**M) Demo: Creating & Training your ML Model**

**III) Deep Learning Foundations (For Absolute Beginners- Optional)**

**A) What is Deep Learning ?**

**B) What is a Neural Network ?**

**C) Deep Learning Models**

**D) What is a Transformer Model?**

**E) Demo: GANs - Deep Fake Video**

**F) Demo- Creating & Training Deep Learning Model**



# **I) AI Concepts and Workloads - (For Absolute Beginners- Optional)**

# **A) What is AI ?**

Hi folks, welcome back. So we need to start with the one-million-dollar question: What is AI? Right now we are looking to go deep into the certifications for artificial intelligence, but what if we actually don’t understand what AI really is? How should we start learning these concepts? That’s why it is very important to first understand what AI actually means. It’s always good to break down the words. So let’s break the term Artificial Intelligence and understand the meaning.

First, what do we mean by artificial? Artificial is something that is not natural. When we talk about something natural, we mean something that exists in nature on its own. But when we talk about something artificial, it is man-made. So the first thing to remember is that artificial refers to something created by humans.

Next, what is intelligence? It is a very simple word, but it carries a deep meaning. Intelligence is the ability to acquire and apply knowledge and skills. So when we combine the two terms, we get Artificial Intelligence. This means intelligence that is man-made, not natural. Humans have natural intelligence, but machines and computers have artificial intelligence.

If we look at the complete definition of AI, it is the ability or capability of a computer system to mimic human-like cognitive functions. Now you might ask, what are cognitive functions? Cognitive mainly involves three things: knowing, learning, and understanding. So when we try to make computers smart enough to have human-like cognitive abilities—where computers can understand, learn, and know things—we are essentially talking about AI. In short, AI is the ability of a computer system to mimic human-like cognitive functions.

In the next part, let’s quickly look at a demo of AI. I thought it would be good to give you a quick example of what AI looks like, especially if you are new to this field. So let’s take a look.

Introducing Phantom—the most advanced chessboard in the world. It brings you the infinite possibilities of online chess with the engaging experience of a physical set. Phantom is also the smartest board ever. It allows you to play against any human on Earth remotely, moving the pieces using only your voice. For example, you can simply say, “Knight G4,” and the board responds. You can also play against its human-like AI, which continuously adapts to your playing level. The pieces can even replay the most famous games in history. Phantom brings back all the little details that make chess great.

This is a perfect example of artificial intelligence. If you noticed, the person controlling the board was simply giving voice commands. In the world of AI, this falls under NLP (Natural Language Processing). You just say “move the knight to G4,” and based on your speech, the piece moves. The system also includes a human-like AI that adapts to your playing level. The chessboard is so intelligent that it can understand whether you are an expert or a beginner simply based on the type of moves you play. Within the first few moves, it can gauge your level and adjust its own strategy accordingly. A perfect demonstration of AI in action.

# **B) History of AI**

So my dad always says that whenever you have to start with a new subject, you should always begin with its history. That’s exactly what I’ve done here. I’ve created a complete timeline of artificial intelligence for you, and you’ll actually be surprised to see that artificial intelligence has existed for ages. We’re talking all the way back to the 1950s. In 1950, a British mathematician named Alan Turing published a groundbreaking paper titled Computing Machinery and Intelligence. This is where the idea of making computers as intelligent as humans truly began. He also introduced the famous Turing Test, a simple method to determine whether a machine can demonstrate human intelligence.

Then we move to 1956, when John McCarthy coined the term Artificial Intelligence at the first-ever AI conference held at Dartmouth College. This is why he is often called the father of artificial intelligence. From there, things began to mature further in the 1960s. In this decade, Frank Rosenblatt built the Mark I Perceptron, the first computer based on a neural network. Don’t worry if you don’t know much about neural networks yet―we will deep dive into them later. For now, think of neural networks as an attempt by scientists to mimic how the human brain works using neurons. These early systems mostly worked on trial and error: if something went wrong, the algorithm was improved and refined.

Then we jump to the 1980s, when neural networks became more mature. This is when the concept of backpropagation emerged. Backpropagation is a gradient estimation method used to train neural networks by adjusting their internal weights. It was a huge step forward in making machine learning more effective and practical.

In 1997, a major milestone shocked the world. IBM’s Deep Blue defeated Garry Kasparov, the reigning world chess champion. It was global news because it was the first time a computer had beaten a world champion in chess, proving how far AI had progressed.

Advancing to 2011, AI had another major moment with the game show Jeopardy! Instead of being asked questions, contestants are given clues in the form of answers and must respond with the correct question. IBM’s Watson competed against champions Ken Jennings and Brad Rutter—and Watson won. It demonstrated the enormous potential of AI in understanding natural language and processing vast amounts of information.

By 2015, things were heating up even more. The Chinese tech giant Baidu introduced Minerva, a supercomputer that used advanced deep neural networks. Its image identification and categorization abilities exceeded the accuracy of the average human. This was one of the early signals that AI could outperform humans in certain cognitive tasks.

In 2016, another historic moment arrived with the ancient board game Go. Go is a complex strategy game where the objective is to capture more territory than the opponent. DeepMind’s AlphaGo, powered by deep neural networks, defeated world champion Lee Sedol in a five-game match. Within those five games, AlphaGo proved that AI could master even the most complex strategy-based human games.

Then we arrive at the 2020s, which I always call the true game changer. AI was always there, but what completely changed its face was Generative AI. OpenAI released GPT-3, one of the world’s most sophisticated language processing models capable of generating human-like text. This is fundamentally different from predictive AI. Generative AI can understand context and create original content—stories, poems, essays, letters—just from a simple prompt. That’s why I say this era is transformative. From 2020 onwards, you will see rapid evolution in this field, and this is where companies are now putting their R&D budgets.

So with this, I think you now have a solid understanding of the entire AI timeline.

# **C) Benefits of Artificial Intelligence (AI)**

Hello and welcome. After understanding what AI is and exploring the AI timeline, it's now time to look into the benefits of artificial intelligence. Why is AI so important, and why are we learning about it today? The key thing to remember is that AI has been around us for many years, as we saw in the timeline, and its impact continues to grow.

One of the most important benefits is no human error. Humans naturally make mistakes, and many major outages or system failures in the real world have happened due to human error. AI systems, on the other hand, are smart and intelligent machines that follow precise instructions and algorithms. Since the tasks are being performed by computers and not humans, the likelihood of errors significantly reduces.

Another major advantage is 24×7 availability. Humans need to eat, sleep, rest, and take breaks in order to function properly, but machines do not. AI systems can work 24 hours a day, 7 days a week, and 365 days a year without getting tired. They are always available and do not require downtime like humans do.

Next is the fact that humans can be biased, whereas machines are not. Humans may be emotionally inclined or unfair due to personal beliefs or preferences. Machines, however, make decisions purely based on the data, algorithms, and models they are trained on, not based on emotions. This leads to more unbiased and consistent decision-making.

AI also enables quicker decision-making. While the human brain is powerful, processing large volumes of data manually can take a lot of time. AI systems excel at parallel processing. You can feed huge amounts of data into a machine, and it can quickly analyze, comprehend, and make decisions much faster than a human could.

AI also helps in reducing risks—maybe not “no risks,” but definitely lesser risks. Since machines do not make human errors and do not carry emotional biases, organizations can significantly lower operational and decision-making risks by relying on AI.

One area where AI has shown tremendous progress is healthcare. AI systems are being used to diagnose diseases, predict health outcomes, and even recommend treatment plans. This has elevated the quality of medical assistance and opened new possibilities in patient care, diagnosis, and precision medicine.

Another key benefit is the ability to manage recurring tasks effectively. In many companies, employees are assigned repetitive daily tasks—like running the same script every morning at 9 AM. Over time, a human will naturally feel bored, frustrated, or unmotivated because the task is mundane. Machines, however, do not complain, do not get bored, and do not experience fatigue. They can perform repetitive tasks consistently and accurately without any emotional response, making them ideal for automation.

While AI offers many more benefits beyond these, the points covered here represent the most essential advantages you should keep in mind as you continue learning about artificial intelligence.

# **D) Types of AI Workloads**

Now that we’ve already covered the fundamentals of AI, it’s time to look at the various workloads that exist within artificial intelligence. When we talk about AI workloads, we are basically referring to different categories or types of problems AI is designed to solve. In this section, I’ll explain each workload clearly, and I’ve also referenced small demo videos or examples to help you understand how these workloads work in the real world.

The first workload is Machine Learning. Don’t worry if you don’t fully understand machine learning yet—we will deep dive into what it is, how it works, and the different types of machine learning later. For now, just remember that machine learning is a branch of artificial intelligence and computer science that uses data and algorithms to imitate the way humans learn and gradually improve accuracy. A simple example is how a child learns to tell the difference between a cat and a dog. When a child is born, they have no idea which is which. But over time, through books, pictures, and guidance from parents, the child learns the features of a cat versus the features of a dog. Machine learning works similarly: we teach a computer model to make predictions and draw conclusions from data, just like humans learn from experience.

A great real-world example is Netflix. When you watch movies on Netflix, the platform recommends new movies or TV shows based on your viewing history. If you watch a lot of thrillers, Netflix learns that and recommends more thrillers. If you prefer romantic comedies, it will show you romcom suggestions. Netflix uses machine learning techniques such as A/B testing to compare algorithms and find out which recommendations bring more viewer satisfaction. Their system continuously learns from user interactions to provide better suggestions.

The next workload is Computer Vision. This is an area of AI that enables computers to identify, detect, and classify objects within images or videos. Until recently, computers could store photos and videos, but they couldn’t actually understand what objects were present in them. With computer vision, machines can visually interpret the world using cameras, images, and video streams. For example, a system can look at a video and identify people, vehicles, objects, or even track movements. If there was a ball in the scene, the computer could detect that it is a ball. This workload forms the basis for technologies like self-driving cars, surveillance systems, medical imaging, and facial recognition.

Another major workload is Natural Language Processing (NLP). As the name suggests, natural language refers to the way humans communicate—whether through spoken or written language. NLP equips computers with the ability to understand human language and respond accordingly. A perfect example is Alexa. You simply talk to Alexa in English, Hindi, or any supported language, and it interprets your speech, processes your request, and responds naturally. Whether you are asking Alexa to solve math problems, make an announcement, or perform a task, you are interacting with an NLP-enabled system that understands your voice commands and replies just like a human would.

Next, we have Generative AI (GenAI), which is one of the biggest technological shifts today. Earlier, AI systems mainly focused on predictive tasks, where the outcome was already defined—for example, predicting whether a person has diabetes based on input data. You feed the data, and the system predicts a yes or no outcome. Generative AI, however, goes much further. Instead of only predicting, it has the ability to create original content. This content can be in the form of text, images, code, diagrams, videos, and much more.

For example, suppose you want to surprise your wife on her birthday by writing a poem but you’re not confident about your writing skills. With generative AI, you simply provide some details—where you met her, what she likes, her positive qualities—and the system will instantly generate a beautiful poem. Similarly, you can ask it to write Python code, create an image, produce an architectural diagram, or generate a detailed explanation. Tools like ChatGPT demonstrate this perfectly. When you ask ChatGPT to write a 400-word review on gas sensor datasets, it produces the content within seconds. The speed and creativity are remarkable, making generative AI a revolutionary leap forward in how we interact with technology.

With this overview, you should now have a clear understanding of the various AI workloads—Machine Learning, Computer Vision, Natural Language Processing, and Generative AI—and how each plays a major role in shaping the world of artificial intelligence.

# **E) AI vs ML vs DL**

Another key question that comes up when studying AI is understanding the difference between Artificial Intelligence, Machine Learning, and Deep Learning—often referred to as AI vs ML vs DL. The best way to visualize this is like an onion with multiple layers. Each layer represents a level of abstraction, with AI at the top, ML in the middle, and DL at the core.

At the topmost layer is Artificial Intelligence (AI). As we’ve discussed, AI is the capability of a computer system to mimic human-like cognitive functions, where cognitive stands for knowing, learning, and understanding. AI is the broadest field and encompasses any technique or system that enables machines to perform tasks that typically require human intelligence.

Beneath AI lies Machine Learning (ML). Machine learning is a subset of AI and a branch of computer science that focuses on using data and algorithms to imitate the way humans learn. ML systems improve their accuracy over time through experience. A simple analogy is a baby learning to distinguish between a cat and a dog. Initially, the baby cannot tell the difference, but through observation, guidance from parents, and books, the child learns to recognize the features of each. A modern example is Netflix, which uses machine learning to recommend movies and shows based on your preferences. The system learns from your interactions and improves its recommendations over time. The key idea here is learning and improving—machine learning systems get better as they process more data.

At the innermost layer is Deep Learning (DL). Deep learning is a subset of machine learning that teaches computers to process data in ways inspired by the human brain, particularly using neural networks. It enables the computation of multi-layer neural networks, making complex tasks feasible. While we will explore neural networks in more detail later, it’s important to understand that deep learning forms the foundation for advanced technologies like driverless cars and generative AI systems. Deep learning allows machines to process vast amounts of unstructured data, recognize patterns, and make complex decisions autonomously.

In summary, the relationship between AI, ML, and DL can be visualized as layers: AI is the topmost layer, encompassing all intelligent behavior; ML sits beneath AI, focusing on systems that learn and improve; and DL forms the core, enabling highly complex tasks through neural networks inspired by the human brain. Understanding this layered mechanism helps clarify the distinctions and connections between these three critical areas of artificial intelligence.

# **II) Machine Learning Foundations (For Absolute Beginners-Optional)**

# **A) Machine Learning - Real World Example**

Okay, so let's take a look at a real-world example of machine learning. So it's just people who are actually working on machine learning at Netflix day in and day out, and they'd be sharing their experiences. So for the moment, just try to have a look. Don't worry. We'll be going into the theoretical part of machine learning just after this video. At Netflix, we have over 120 million members. They span the whole globe across 190 different countries, as well as a diversity of titles, content, comedies, and dramas. Machine learning is deeply intertwined with all aspects of Netflix's business—how we compose our catalog of content, how we produce our content, how we encode it, how we stream it. We use machine learning to help marketing efforts and advertising efforts. Then we also use machine learning in our content acquisition effort. Okay, so this was just a quick snippet or a video on machine learning in the real world. In the next video, we'll start learning about some of the key terminologies of machine learning because once you get a good understanding of the key terminologies, then you can have a good hold of the machine learning fundamentals. Thanks for watching.

# **B) Machine Learning - Key Terminologies**

Hi folks. Welcome back. So now it's time to go into the theoretical definitions and the different terminologies for machine learning. Before we actually do a deep dive into machine learning, I thought it would be better to give you a good understanding of different terminologies. This is very important because whenever you are reading about machine learning, artificial intelligence, or when you're talking to AI professionals or data scientists, these are the kind of words they will keep on saying. If you don't have a good understanding of these words or terminologies, you'll really struggle in the field of machine learning or AI. So let's take a look at these fundamental concepts. The very first one is an algorithm. What is an algorithm? A general definition of an algorithm, when we studied math or physics, is that an algorithm always refers to a set of rules. 

In machine learning, an algorithm refers to a set of rules and statistical techniques. Now you might remember what statistics is. A clear definition of statistics is that it is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data. In other words, you have a lot of data, and from that data, you need to draw conclusions. So algorithm is a set of rules and statistical techniques. A good example of this is the decision tree algorithm. A perfect example of that is Gmail. When Gmail came, it never had a concept of filters. After some time, they came up with the concept of filters to classify emails as spam or not spam. They look at your emails and, based on a certain set of rules, they tell you which emails are spam and which are not. They do this based on features—now called features or artifacts in machine learning—such as who the sender is, frequency of certain words, or specific phrases like "you have won a lottery" or "there is money in your account." These features help the algorithm decide whether an email is spam or not. This is your algorithm. The next concept is a model. A model is what an algorithm creates. 

Always remember, a model is derived from an algorithm. A model is essentially the learned representation of data. It is a program that can find patterns and make decisions. For example, a dataset of house features and prices fed into a regression algorithm can create a model that predicts the price of new houses based on their features. If you feed new data to the model, it can predict price ranges for houses. So a model is an algorithm that has been trained on data, and it helps find patterns to make decisions. Now comes the concept of training. You always train your model, similar to training a child to make decisions. Training is the process of presenting data to a machine learning algorithm to create a model. The algorithm uses training data to learn. For example, a neural network shown thousands of pictures of cats and dogs can learn to identify whether a new image is a cat or a dog based on features like ears, eyes, and other patterns. Then, there's the concept of labels. A label is the output you want the model to predict. For example, in healthcare, the label could be whether a tumor is malignant or benign. Malignant means dangerous, while benign means it is not a major issue. To summarize, algorithm is a set of rules, a model is what an algorithm creates after being trained on data, training is the process of presenting data to an algorithm to create a model, and labels are the outputs the model is trying to predict. Thanks for watching.

# **C) What is Machine Learning ?**

Hello and welcome. So the time has finally come to talk about machine learning. Before we go into the details of what machine learning is, it’s important to understand how humans learn. Just think about it—when a baby is born, can the baby distinguish between a cat, a dog, and a horse? No. A baby learns by observing different features. For example, if an animal has long ears and a black nose, it might be a dog. If it has small size and blue eyes, it could be a cat. Humans learn primarily by examples, diagrams, and comparisons. Similarly, we train machines to learn using data examples.

For instance, if we feed thousands of labeled photographs of dogs, cats, and horses to a computer, it can identify patterns based on features like ear length, nose shape, and eye color. These patterns are used to train an algorithm, and once the model is built based on that algorithm, it can make predictions on new data. This model can then classify a new image as a cat, dog, or horse based on its features.

Theoretically, machine learning is a branch of artificial intelligence. As discussed earlier in the AI-ML-DL “onion” analogy, machine learning (ML) is a subset of artificial intelligence (AI). ML focuses on the use of data and algorithms. If I were to define the heart and soul of machine learning, it would be data. Machine learning helps machines imitate how humans learn, gradually improving their accuracy. Computers apply statistical learning techniques to automatically identify patterns in data. Statistics itself is the study and manipulation of data, including ways to gather, review, analyze, and draw conclusions from data.

Data contains patterns, and algorithms are used to find these patterns. Using our cat and dog example, features in the data are analyzed to find patterns, which are then used to train the algorithm. Once trained, a model is created. This model can recognize patterns in new data. Data is typically divided into training data, which is used to train the model, and testing data, which is used to evaluate its accuracy. The model is then used to make predictions on new, unseen data.

As machine learning progresses, increased data and experience improve the accuracy of the results, similar to how humans learn gradually. Machine learning algorithms improve as they process more data, updating their parameters to learn over time. There are three main approaches to machine learning: supervised learning, unsupervised learning, and reinforcement learning.

Supervised learning involves labeled data. For example, in the cat and dog dataset, features are labeled to indicate whether the animal is a cat or a dog. Unsupervised learning deals with unlabeled data and helps identify patterns or structure in such data. Reinforcement learning is based on rewards and penalties. Similar to how a teacher rewards a student for correct actions and penalizes mistakes, machines learn by maximizing rewards and minimizing penalties over time.

Data is the foundation of machine learning. Both the quality and quantity of data are critical. Garbage data will result in poor models, while high-quality data improves accuracy. Likewise, larger datasets help models better understand patterns and improve predictions. For example, feeding 100 rows of data will train a model to some extent, but 10,000 rows will give the model a much better understanding of the patterns.

Algorithms in machine learning are sets of rules, often based on statistical or mathematical techniques. They are tailored to specific types of data and learning tasks. As seen in real-world examples like Netflix, machine learning is applied across diverse fields, including healthcare, finance (e.g., fraud detection), autonomous vehicles, and tasks like prediction, classification, clustering, and deployment. Once trained, models are deployed in real-world applications to provide insights, automate tasks, or enhance decision-making.

In conclusion, machine learning is essentially about teaching machines to learn like humans. By feeding data, finding patterns, and building models, machines can make predictions and decisions. Data-centric machine learning relies heavily on the quality and quantity of data. Always remember—think of machine learning as training a baby: the better and more comprehensive the training data, the better the machine learns.

# **D) Types of Machine Learning**

Okay, so now we have got a very good understanding of what machine learning is. We have seen some real-world examples of machine learning, and we also discussed the workflow of machine learning. Now, it is important to understand the various types of machine learning algorithms. In this section, we will do a deep dive into each of these types.

First, we will learn about supervised machine learning. Supervised learning is where the model training is done using labeled data. Don’t worry—we will explore this in detail, and we will understand concepts like classification and regression.

Next, we will talk about unsupervised learning. Unsupervised learning is called “unsupervised” because it uses unlabeled data. In unsupervised learning, we have the concept of clustering. Each of these algorithms or models addresses different use cases, which we will also discuss in detail later.

Then comes reinforcement learning. As discussed earlier, reinforcement learning works based on the reward and punishment method. Just like a baby or a student learns by receiving rewards for correct actions and penalties for mistakes, reinforcement learning works on a feedback loop. The model takes actions in an environment, receives feedback and state updates, and gradually learns to make better decisions.

So, with this quick overview, we can summarize that there are mainly three types of machine learning algorithms: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning includes classification and regression, unsupervised learning mainly focuses on clustering, and reinforcement learning relies on feedback from the environment.

In the next video, we will go deeper into supervised learning. See you in the video. Thanks for watching.

# **E) What is Supervised Machine Learning ?**

Hello and welcome. It's time to take a look into supervised machine learning. Whenever we learn a new subject, it helps to look at its root word. In this case, the root word is “supervisor.” A supervisor is a person in charge of a group, ensuring that work is done correctly, accurately, and according to the rules.

The term supervised machine learning comes from the fact that the algorithm is trained using labeled data. Labeled datasets are annotated with meaningful tags or labels that classify the outcome. For example, in our earlier cat and dog example, we gave features such as nose type, ear type, and ear length, and labeled the animal as either a cat or a dog. In this case, the labeled dataset acts as the supervisor—it guides the algorithm in learning the correct output.

Supervised learning involves training a model on a dataset that includes both input data and the corresponding correct output. A classic example is the Iris flower dataset, which contains various types of Iris flowers, such as Setosa, Versicolor, and Virginica. Each flower has features like sepal length, sepal width, petal length, and petal width. The input data consists of these features, while the output data is the flower’s class. By providing thousands of such examples, the model can learn patterns in the data and make accurate predictions for new inputs.

The training dataset must be labeled, meaning each example is paired with the correct answer. For instance, if you provide sepal and petal measurements along with the corresponding flower type, the model learns to map the input features to the correct class. This mapping allows the model to make predictions on unseen data.

There are different types of algorithms in supervised machine learning, including linear regression, logistic regression, support vector machines (SVMs), decision trees, and neural networks (the latter being explored further in deep learning). The training process is straightforward: the model learns a function that maps inputs to the desired outputs and makes predictions based on that function. For example, if you feed a model new data such as sepal length 5.9, sepal width 3.0, petal length 5.0, and petal width 1.8, it should predict that the flower is a Virginica.

One risk in supervised machine learning is overfitting, where the model performs very well on training data but gives inaccurate results for unseen data. Overfitting occurs when the model learns patterns that are too specific to the training set, reducing its ability to generalize.

Supervised machine learning has a wide range of applications. Examples include image recognition, where models identify cats and dogs in images; speech recognition, such as voice-based banking systems where the model identifies a user’s voice as a password; medical diagnosis, where models help identify diseases; spam detection, which classifies emails as spam or not spam; and stock price prediction, which forecasts continuous values.

In supervised learning, there are two main types: classification, which sorts items into categories, and regression, which predicts continuous values. For example, classification can label an email as spam or ham, while regression can predict a house price based on features. We will explore these types in more detail in the next videos, looking at the differences between classification and regression.

# **F) What is Classification in SML ?**

# **G) What is Regression in SML ?**

# **H) What is Unsupervised Machine Learning ?**

# **I) What is Re-inforcement Machine Learning**

# **J) What is a Jupyter Notebook**

# **K) Demo: Install Anaconda**

# **L) Demo: Understanding the IRIS Dataset**

# **M) Demo: Creating & Training your ML Model**

# **III) Deep Learning Foundations (For Absolute Beginners- Optional)**

# **A) What is Deep Learning ?**

# **B) What is a Neural Network ?**

# **C) Deep Learning Models**

# **D) What is a Transformer Model?**

# **E) Demo: GANs - Deep Fake Video**

# **F) Demo- Creating & Training Deep Learning Model**

